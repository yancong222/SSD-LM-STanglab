{"cells":[{"cell_type":"markdown","metadata":{},"source":["# note: This program needs openAI API"]},{"cell_type":"markdown","metadata":{"id":"MXUX3Z1dJOcw"},"source":["# Prep dataset"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ZuHPbrbAJOcy"},"outputs":[{"data":{"text/plain":["'/Users/yancong/Desktop/4 clinical/02 projects_parsely/05 ssd-lm-stanglab/15 simulation/GPT3simul'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import scipy\n","from torch import tensor\n","\n","os.getcwd()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>grid</th>\n","      <th>content</th>\n","      <th>n_words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10455</td>\n","      <td>I'm a young man , an en an en- an engineer by ...</td>\n","      <td>421</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11689</td>\n","      <td>Sure . I'm thirty three years old . My name is...</td>\n","      <td>159</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12376</td>\n","      <td>Alright . um I live in not especially cool Spr...</td>\n","      <td>468</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12630</td>\n","      <td>um So I'm currently twenty-nine . I was born a...</td>\n","      <td>966</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>13493</td>\n","      <td>Mhm . I'm a thirty five year old man who uh um...</td>\n","      <td>134</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    grid                                            content  n_words\n","0  10455  I'm a young man , an en an en- an engineer by ...      421\n","1  11689  Sure . I'm thirty three years old . My name is...      159\n","2  12376  Alright . um I live in not especially cool Spr...      468\n","3  12630  um So I'm currently twenty-nine . I was born a...      966\n","4  13493  Mhm . I'm a thirty five year old man who uh um...      134"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# reading in data by sample\n","# each sample is collapsed from the two open-ended prompts\n","\n","data_foloder = '/Gradient_shuffle/'\n","result = '/Analysis/'\n","baseline = pd.read_csv(data_foloder + 'simulation_HV_baseline_vb_response_deid_v3.csv')\n","incoh10 = pd.read_csv(data_foloder + 'simulation_HV_incoh_vb_response_deid_10v3.csv')\n","incoh20 = pd.read_csv(data_foloder + 'simulation_HV_incoh_vb_response_deid_20v3.csv')\n","incoh50 = pd.read_csv(data_foloder + 'simulation_HV_incoh_vb_response_deid_50v3.csv')\n","ineff10 = pd.read_csv(data_foloder + 'simulation_HV_ineff_vb_response_deid_10v3.csv')\n","ineff20 = pd.read_csv(data_foloder + 'simulation_HV_ineff_vb_response_deid_20v3.csv')\n","ineff50 = pd.read_csv(data_foloder + 'simulation_HV_ineff_vb_response_deid_50v3.csv')\n","\n","baseline.head() "]},{"cell_type":"markdown","metadata":{},"source":["# Install lib and dependencies"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Rmelhcu1JOc2","outputId":"31e9eed6-24ab-4a28-d4a2-0607f26f899e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (4.8.1)\n","Requirement already satisfied: numpy>=1.17 in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from transformers) (1.20.3)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from transformers) (0.0.12)\n","Requirement already satisfied: requests in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from transformers) (2.26.0)\n","Requirement already satisfied: pyyaml in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from transformers) (2021.8.3)\n","Requirement already satisfied: sacremoses in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from transformers) (0.0.47)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from transformers) (0.10.3)\n","Requirement already satisfied: packaging in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from transformers) (4.62.3)\n","Requirement already satisfied: filelock in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from transformers) (3.4.0)\n","Requirement already satisfied: typing-extensions in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from huggingface-hub==0.0.12->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from packaging->transformers) (3.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from requests->transformers) (3.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: joblib in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /Users/yancong/opt/anaconda3/envs/nlum1/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n","\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n","You should consider upgrading via the '/Users/yancong/opt/anaconda3/envs/nlum1/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# no need to install it every time you open it\n","# for GPT-3\n","# install GPT-3 from huggingface\n","!pip install transformers "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"jf7X4aosJOc2"},"outputs":[],"source":["# import tokenizers from GPT2TokenizerFast (GPT-3 uses the same tokenizer)\n","\n","import transformers\n","from transformers import GPT2TokenizerFast\n","tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"-biYC5D0JOc5"},"outputs":[],"source":["# use openai API\n","from openai.embeddings_utils import get_embeddings, cosine_similarity # important\n","import openai\n","openai.api_key = 'your key'"]},{"cell_type":"markdown","metadata":{},"source":["# Get word embeddings from whole response embeddings GPT3 3df"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["current grid:  10455\n","current grid:  11689\n","current grid:  13493\n","current grid:  10455\n","current grid:  11689\n","current grid:  13493\n","current grid:  10455\n","current grid:  11689\n","current grid:  13493\n","current grid:  10455\n","current grid:  11689\n","current grid:  13493\n","current grid:  10455\n","current grid:  11689\n","current grid:  13493\n","current grid:  10455\n","current grid:  11689\n","current grid:  13493\n","current grid:  10455\n","current grid:  11689\n","current grid:  13493\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>grid</th>\n","      <th>content</th>\n","      <th>n_words</th>\n","      <th>gpt3_embed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10455</td>\n","      <td>I'm a young man , an en an en- an engineer by ...</td>\n","      <td>421</td>\n","      <td>[[0.009009403176605701, -0.017146145924925804,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11689</td>\n","      <td>We have been using that opportunity to do more...</td>\n","      <td>159</td>\n","      <td>[[-0.00924572255462408, -0.014978389255702496,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12376</td>\n","      <td>Alright . um I live in not especially cool Spr...</td>\n","      <td>468</td>\n","      <td>[[0.010747547261416912, -0.01882629469037056, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12630</td>\n","      <td>My is things are fantastic . No , I mean My uh...</td>\n","      <td>966</td>\n","      <td>[[-0.012155817821621895, -0.022000038996338844...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>13493</td>\n","      <td>Mhm . I still get to play . And my stock inves...</td>\n","      <td>134</td>\n","      <td>[[0.003884142730385065, -0.01114651095122099, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    grid                                            content  n_words  \\\n","0  10455  I'm a young man , an en an en- an engineer by ...      421   \n","1  11689  We have been using that opportunity to do more...      159   \n","2  12376  Alright . um I live in not especially cool Spr...      468   \n","3  12630  My is things are fantastic . No , I mean My uh...      966   \n","4  13493  Mhm . I still get to play . And my stock inves...      134   \n","\n","                                          gpt3_embed  \n","0  [[0.009009403176605701, -0.017146145924925804,...  \n","1  [[-0.00924572255462408, -0.014978389255702496,...  \n","2  [[0.010747547261416912, -0.01882629469037056, ...  \n","3  [[-0.012155817821621895, -0.022000038996338844...  \n","4  [[0.003884142730385065, -0.01114651095122099, ...  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["buglist = {} # initialize dic to log bugs\n","def calc_gpt3(r): # defining function for extracting contextualized embeddings from verbatim transcripts\n","  if r[0] % 2 != 0: # keeping track of progress - print out even grid as we process them\n","    print('current grid: ', r[0])\n","  if type(r['content']) == str and r['n_words'] < 2048: # if the content is string [non-empty]; 2048 is max for GPT-3\n","    temp = r['content'].split(' ') # gives a list of words\n","    input = ' '.join([i for i in temp if (i != '') & ('{' not in i) & ('}' not in i) & ('#' not in i)]) # exclude NSV and the restart symbol\n","    try:\n","      vec = get_embeddings(input.split(' '), engine = 'text-similarity-babbage-001') # take in the whole response as a list of words\n","    except:\n","      buglist[r[0]] = input.split(' ') # store grid if there is a bug\n","    return vec\n","  else:\n","    return np.nan\n","\n","# double check your input df\n","# make sure there is no '  ' or '   '\n","# put together all the data frames into 1 list\n","# to avoid repeated work\n","dfs = [baseline, incoh10, incoh20, incoh50, ineff10, ineff20, ineff50]\n","temp = -1\n","for df in dfs:\n","  temp += 1\n","  df[\"gpt3_embed\"] = ''\n","  df[\"gpt3_embed\"] = df.apply(lambda r: calc_gpt3(r), axis = 1) \n","  df.to_csv(result + str(temp) + '.csv')\n","\n","df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["{}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["buglist"]},{"cell_type":"markdown","metadata":{},"source":["# MV 5/10"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['1', '5', '99', '34', '109'], ['gh', 'io', 'wer', '90', '901'], ['98', 'iop', 'er4', 'op0']]\n"]}],"source":["# Average semantic similarity of each word in 5- or 10- words window\n","\n","def divide_chunks(l, n):\n","      \n","    # looping till length l\n","    for i in range(0, len(l), n): \n","        yield l[i:i + n]\n","  \n","# n: How many elements each\n","# list should have\n","test = ['1','5','99','34','109','gh','io','wer','90','901','98','iop','er4','op0']\n","divide_chunks(test,5)\n","print(list(divide_chunks(test,5)))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["current:  gpt3_word_mv5\n","current:  gpt3_word_mv10\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/xl/6x4fjht11dq2blkl91wqmnkh0000gn/T/ipykernel_45904/3508527720.py:30: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['similarity'][i] = chunk_temp_sum\n","/var/folders/xl/6x4fjht11dq2blkl91wqmnkh0000gn/T/ipykernel_45904/3508527720.py:32: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[cur][i] = sim\n","/var/folders/xl/6x4fjht11dq2blkl91wqmnkh0000gn/T/ipykernel_45904/3508527720.py:28: RuntimeWarning: Mean of empty slice\n","  temp_sim = np.nanmean(temp_sum)\n"]},{"name":"stdout","output_type":"stream","text":["current:  gpt3_word_mv5\n","current:  gpt3_word_mv10\n","current:  gpt3_word_mv5\n","current:  gpt3_word_mv10\n","current:  gpt3_word_mv5\n","current:  gpt3_word_mv10\n","current:  gpt3_word_mv5\n","current:  gpt3_word_mv10\n","current:  gpt3_word_mv5\n","current:  gpt3_word_mv10\n","current:  gpt3_word_mv5\n","current:  gpt3_word_mv10\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>grid</th>\n","      <th>content</th>\n","      <th>n_words</th>\n","      <th>gpt3_embed</th>\n","      <th>similarity</th>\n","      <th>gpt3_word_mv5</th>\n","      <th>gpt3_word_mv10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10455</td>\n","      <td>I'm a young man , an en an en- an engineer by ...</td>\n","      <td>421</td>\n","      <td>[[0.009009403176605701, -0.017146145924925804,...</td>\n","      <td>[0.8556107616829599, 0.7243433395212227, 0.780...</td>\n","      <td>0.776858</td>\n","      <td>0.777624</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11689</td>\n","      <td>We have been using that opportunity to do more...</td>\n","      <td>159</td>\n","      <td>[[-0.00924572255462408, -0.014978389255702496,...</td>\n","      <td>[0.8087813300097076, 0.8080145213914288, 0.788...</td>\n","      <td>0.786263</td>\n","      <td>0.783373</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12376</td>\n","      <td>Alright . um I live in not especially cool Spr...</td>\n","      <td>468</td>\n","      <td>[[0.010747547261416912, -0.01882629469037056, ...</td>\n","      <td>[0.7559491116579808, 0.7876836248597551, 0.776...</td>\n","      <td>0.784385</td>\n","      <td>0.785468</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12630</td>\n","      <td>My is things are fantastic . No , I mean My uh...</td>\n","      <td>966</td>\n","      <td>[[-0.012155817821621895, -0.022000038996338844...</td>\n","      <td>[0.7832724098667, 0.8118012931973864, 0.756688...</td>\n","      <td>0.787379</td>\n","      <td>0.787312</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>13493</td>\n","      <td>Mhm . I still get to play . And my stock inves...</td>\n","      <td>134</td>\n","      <td>[[0.003884142730385065, -0.01114651095122099, ...</td>\n","      <td>[0.8039042835152086, 0.7656657122352821, 0.743...</td>\n","      <td>0.779932</td>\n","      <td>0.780051</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    grid                                            content  n_words  \\\n","0  10455  I'm a young man , an en an en- an engineer by ...      421   \n","1  11689  We have been using that opportunity to do more...      159   \n","2  12376  Alright . um I live in not especially cool Spr...      468   \n","3  12630  My is things are fantastic . No , I mean My uh...      966   \n","4  13493  Mhm . I still get to play . And my stock inves...      134   \n","\n","                                          gpt3_embed  \\\n","0  [[0.009009403176605701, -0.017146145924925804,...   \n","1  [[-0.00924572255462408, -0.014978389255702496,...   \n","2  [[0.010747547261416912, -0.01882629469037056, ...   \n","3  [[-0.012155817821621895, -0.022000038996338844...   \n","4  [[0.003884142730385065, -0.01114651095122099, ...   \n","\n","                                          similarity gpt3_word_mv5  \\\n","0  [0.8556107616829599, 0.7243433395212227, 0.780...      0.776858   \n","1  [0.8087813300097076, 0.8080145213914288, 0.788...      0.786263   \n","2  [0.7559491116579808, 0.7876836248597551, 0.776...      0.784385   \n","3  [0.7832724098667, 0.8118012931973864, 0.756688...      0.787379   \n","4  [0.8039042835152086, 0.7656657122352821, 0.743...      0.779932   \n","\n","  gpt3_word_mv10  \n","0       0.777624  \n","1       0.783373  \n","2       0.785468  \n","3       0.787312  \n","4       0.780051  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["temp_file = -1 # keep track of intermediate output files names\n","\n","for df in dfs:\n","    temp_file += 1 \n","    \n","    df['similarity'] = '' # create new empty columns\n","    df['gpt3_word_mv5'] = ''\n","    df['gpt3_word_mv10'] = ''\n","\n","    ks = ['5', '10']\n","    for k in ks:\n","        # print progress\n","        cur = 'gpt3_word_mv' + k \n","        print('current: ', cur)\n","        # loop over each response\n","        for i in df.index:\n","            if type(df['gpt3_embed'][i]) != float: \n","                # chop 1 big response sequence into 5/10-token chunks\n","                word_embed_chunk = list(divide_chunks(df['gpt3_embed'][i], int(k)))\n","                chunk_temp_sum = [] \n","                # loop over each chunk in the response\n","                for chunck_id, word_embed in enumerate(word_embed_chunk):\n","                    temp_sum = []\n","                    # calculate average similarities for that chunk\n","                    for word_id, embed in enumerate(word_embed):\n","                        w1 = embed\n","                        try:\n","                            w2 = word_embed[word_id+1]\n","                        except IndexError:\n","                            continue\n","                        temp = cosine_similarity(w1, w2)\n","                        temp_sum.append(temp)\n","                    temp_sim = np.nanmean(temp_sum)\n","                    chunk_temp_sum.append(temp_sim) # incrementally append similarity mean to the list \n","\n","            # get a list of similarity means for that response, \n","            # its len is the number of chunks that the response can be chopped into\n","            df['similarity'][i] = chunk_temp_sum \n","\n","            # add other stats here\n","            sim = np.nanmean(chunk_temp_sum)\n","            df[cur][i] = sim\n","    df.to_csv(result + str(temp_file) + '.csv')\n","df.head()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/xl/6x4fjht11dq2blkl91wqmnkh0000gn/T/ipykernel_45904/463216634.py:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['grid'] = df['grid'].astype(str)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>grid</th>\n","      <th>gpt3_word_mv5</th>\n","      <th>gpt3_word_mv10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10455</td>\n","      <td>0.776858</td>\n","      <td>0.777624</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11689</td>\n","      <td>0.786263</td>\n","      <td>0.783373</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12376</td>\n","      <td>0.784385</td>\n","      <td>0.785468</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12630</td>\n","      <td>0.787379</td>\n","      <td>0.787312</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>13493</td>\n","      <td>0.779932</td>\n","      <td>0.780051</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    grid gpt3_word_mv5 gpt3_word_mv10\n","0  10455      0.776858       0.777624\n","1  11689      0.786263       0.783373\n","2  12376      0.784385       0.785468\n","3  12630      0.787379       0.787312\n","4  13493      0.779932       0.780051"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["temp = -1\n","for df in dfs:\n","    temp += 1\n","    # get rid of the embeddings columns\n","    # because it's too big\n","    df = df[['grid',\n","       'gpt3_word_mv5', 'gpt3_word_mv10']]\n","    df['grid'] = df['grid'].astype(str)\n","    df.to_csv(result + str(temp) + '.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# K2:10"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>grid</th>\n","      <th>content</th>\n","      <th>n_words</th>\n","      <th>gpt3_embed</th>\n","      <th>gpt3_word_mv5</th>\n","      <th>gpt3_word_mv10</th>\n","      <th>gpt3_word_k2</th>\n","      <th>gpt3_word_k3</th>\n","      <th>gpt3_word_k4</th>\n","      <th>gpt3_word_k5</th>\n","      <th>gpt3_word_k6</th>\n","      <th>gpt3_word_k7</th>\n","      <th>gpt3_word_k8</th>\n","      <th>gpt3_word_k9</th>\n","      <th>gpt3_word_k10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10455</td>\n","      <td>I'm a young man , an en an en- an engineer by ...</td>\n","      <td>421</td>\n","      <td>[[0.009009403176605701, -0.017146145924925804,...</td>\n","      <td>0.776863</td>\n","      <td>0.777622</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11689</td>\n","      <td>We have been using that opportunity to do more...</td>\n","      <td>159</td>\n","      <td>[[-0.00924572255462408, -0.014978389255702496,...</td>\n","      <td>0.786322</td>\n","      <td>0.783411</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12376</td>\n","      <td>Alright . um I live in not especially cool Spr...</td>\n","      <td>468</td>\n","      <td>[[0.010747547261416912, -0.01882629469037056, ...</td>\n","      <td>0.784362</td>\n","      <td>0.785444</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12630</td>\n","      <td>My is things are fantastic . No , I mean My uh...</td>\n","      <td>966</td>\n","      <td>[[-0.012155817821621895, -0.022000038996338844...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>13493</td>\n","      <td>Mhm . I still get to play . And my stock inves...</td>\n","      <td>134</td>\n","      <td>[[0.003884142730385065, -0.01114651095122099, ...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    grid                                            content  n_words  \\\n","0  10455  I'm a young man , an en an en- an engineer by ...      421   \n","1  11689  We have been using that opportunity to do more...      159   \n","2  12376  Alright . um I live in not especially cool Spr...      468   \n","3  12630  My is things are fantastic . No , I mean My uh...      966   \n","4  13493  Mhm . I still get to play . And my stock inves...      134   \n","\n","                                          gpt3_embed gpt3_word_mv5  \\\n","0  [[0.009009403176605701, -0.017146145924925804,...      0.776863   \n","1  [[-0.00924572255462408, -0.014978389255702496,...      0.786322   \n","2  [[0.010747547261416912, -0.01882629469037056, ...      0.784362   \n","3  [[-0.012155817821621895, -0.022000038996338844...           NaN   \n","4  [[0.003884142730385065, -0.01114651095122099, ...           NaN   \n","\n","  gpt3_word_mv10 gpt3_word_k2 gpt3_word_k3 gpt3_word_k4 gpt3_word_k5  \\\n","0       0.777622                                                       \n","1       0.783411                                                       \n","2       0.785444                                                       \n","3            NaN                                                       \n","4            NaN                                                       \n","\n","  gpt3_word_k6 gpt3_word_k7 gpt3_word_k8 gpt3_word_k9 gpt3_word_k10  \n","0                                                                    \n","1                                                                    \n","2                                                                    \n","3                                                                    \n","4                                                                    "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import ast # a module that evaluates mathematical expressions and statements\n","\n","# this might take a while\n","# do not run 2 scripts at once, when using vscode\n","temp = -1\n","ks=['2', '3', '4', '5', '6', '7', '8', '9', '10']\n","\n","for df in dfs:\n","    temp += 1\n","    # create new empty columns\n","    df['gpt3_word_k2'] = ''\n","    df['gpt3_word_k3'] = ''\n","    df['gpt3_word_k4'] = ''\n","    df['gpt3_word_k5'] = ''\n","    df['gpt3_word_k6'] = ''\n","    df['gpt3_word_k7'] = ''\n","    df['gpt3_word_k8'] = ''\n","    df['gpt3_word_k9'] = ''\n","    df['gpt3_word_k10'] = ''\n","    df.to_csv(result + str(temp) + '.csv')\n","df.head()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Coherence k  2\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/xl/6x4fjht11dq2blkl91wqmnkh0000gn/T/ipykernel_2423/4014079027.py:22: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[cur][i] = np.average(temp)\n"]},{"name":"stdout","output_type":"stream","text":["Coherence k  3\n","Coherence k  4\n","Coherence k  5\n","Coherence k  6\n","Coherence k  7\n","Coherence k  8\n","Coherence k  9\n","Coherence k  10\n","Coherence k  2\n","Coherence k  3\n","Coherence k  4\n","Coherence k  5\n","Coherence k  6\n","Coherence k  7\n","Coherence k  8\n","Coherence k  9\n","Coherence k  10\n","Coherence k  2\n","Coherence k  3\n","Coherence k  4\n","Coherence k  5\n","Coherence k  6\n","Coherence k  7\n","Coherence k  8\n","Coherence k  9\n","Coherence k  10\n","Coherence k  2\n","Coherence k  3\n","Coherence k  4\n","Coherence k  5\n","Coherence k  6\n","Coherence k  7\n","Coherence k  8\n","Coherence k  9\n","Coherence k  10\n","Coherence k  2\n","Coherence k  3\n","Coherence k  4\n","Coherence k  5\n","Coherence k  6\n","Coherence k  7\n","Coherence k  8\n","Coherence k  9\n","Coherence k  10\n","Coherence k  2\n","Coherence k  3\n","Coherence k  4\n","Coherence k  5\n","Coherence k  6\n","Coherence k  7\n","Coherence k  8\n","Coherence k  9\n","Coherence k  10\n","Coherence k  2\n","Coherence k  3\n","Coherence k  4\n","Coherence k  5\n","Coherence k  6\n","Coherence k  7\n","Coherence k  8\n","Coherence k  9\n","Coherence k  10\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>grid</th>\n","      <th>content</th>\n","      <th>n_words</th>\n","      <th>gpt3_embed</th>\n","      <th>gpt3_word_mv5</th>\n","      <th>gpt3_word_mv10</th>\n","      <th>gpt3_word_k2</th>\n","      <th>gpt3_word_k3</th>\n","      <th>gpt3_word_k4</th>\n","      <th>gpt3_word_k5</th>\n","      <th>gpt3_word_k6</th>\n","      <th>gpt3_word_k7</th>\n","      <th>gpt3_word_k8</th>\n","      <th>gpt3_word_k9</th>\n","      <th>gpt3_word_k10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10455</td>\n","      <td>I'm a young man , an en an en- an engineer by ...</td>\n","      <td>421</td>\n","      <td>[[0.009009403176605701, -0.017146145924925804,...</td>\n","      <td>0.776863</td>\n","      <td>0.777622</td>\n","      <td>0.782686</td>\n","      <td>0.784116</td>\n","      <td>0.781053</td>\n","      <td>0.782103</td>\n","      <td>0.780552</td>\n","      <td>0.784993</td>\n","      <td>0.778847</td>\n","      <td>0.779661</td>\n","      <td>0.777854</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11689</td>\n","      <td>We have been using that opportunity to do more...</td>\n","      <td>159</td>\n","      <td>[[-0.00924572255462408, -0.014978389255702496,...</td>\n","      <td>0.786322</td>\n","      <td>0.783411</td>\n","      <td>0.789297</td>\n","      <td>0.78867</td>\n","      <td>0.779366</td>\n","      <td>0.784245</td>\n","      <td>0.785857</td>\n","      <td>0.786791</td>\n","      <td>0.782155</td>\n","      <td>0.786047</td>\n","      <td>0.78308</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12376</td>\n","      <td>Alright . um I live in not especially cool Spr...</td>\n","      <td>468</td>\n","      <td>[[0.010747547261416912, -0.01882629469037056, ...</td>\n","      <td>0.784362</td>\n","      <td>0.785444</td>\n","      <td>0.789207</td>\n","      <td>0.785062</td>\n","      <td>0.785558</td>\n","      <td>0.788664</td>\n","      <td>0.78434</td>\n","      <td>0.790977</td>\n","      <td>0.785502</td>\n","      <td>0.791028</td>\n","      <td>0.783467</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12630</td>\n","      <td>My is things are fantastic . No , I mean My uh...</td>\n","      <td>966</td>\n","      <td>[[-0.012155817821621895, -0.022000038996338844...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.791977</td>\n","      <td>0.790015</td>\n","      <td>0.789015</td>\n","      <td>0.788958</td>\n","      <td>0.78967</td>\n","      <td>0.791126</td>\n","      <td>0.789266</td>\n","      <td>0.786787</td>\n","      <td>0.788519</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>13493</td>\n","      <td>Mhm . I still get to play . And my stock inves...</td>\n","      <td>134</td>\n","      <td>[[0.003884142730385065, -0.01114651095122099, ...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.785416</td>\n","      <td>0.795869</td>\n","      <td>0.78943</td>\n","      <td>0.786664</td>\n","      <td>0.786584</td>\n","      <td>0.788698</td>\n","      <td>0.781715</td>\n","      <td>0.785351</td>\n","      <td>0.787932</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    grid                                            content  n_words  \\\n","0  10455  I'm a young man , an en an en- an engineer by ...      421   \n","1  11689  We have been using that opportunity to do more...      159   \n","2  12376  Alright . um I live in not especially cool Spr...      468   \n","3  12630  My is things are fantastic . No , I mean My uh...      966   \n","4  13493  Mhm . I still get to play . And my stock inves...      134   \n","\n","                                          gpt3_embed gpt3_word_mv5  \\\n","0  [[0.009009403176605701, -0.017146145924925804,...      0.776863   \n","1  [[-0.00924572255462408, -0.014978389255702496,...      0.786322   \n","2  [[0.010747547261416912, -0.01882629469037056, ...      0.784362   \n","3  [[-0.012155817821621895, -0.022000038996338844...           NaN   \n","4  [[0.003884142730385065, -0.01114651095122099, ...           NaN   \n","\n","  gpt3_word_mv10 gpt3_word_k2 gpt3_word_k3 gpt3_word_k4 gpt3_word_k5  \\\n","0       0.777622     0.782686     0.784116     0.781053     0.782103   \n","1       0.783411     0.789297      0.78867     0.779366     0.784245   \n","2       0.785444     0.789207     0.785062     0.785558     0.788664   \n","3            NaN     0.791977     0.790015     0.789015     0.788958   \n","4            NaN     0.785416     0.795869      0.78943     0.786664   \n","\n","  gpt3_word_k6 gpt3_word_k7 gpt3_word_k8 gpt3_word_k9 gpt3_word_k10  \n","0     0.780552     0.784993     0.778847     0.779661      0.777854  \n","1     0.785857     0.786791     0.782155     0.786047       0.78308  \n","2      0.78434     0.790977     0.785502     0.791028      0.783467  \n","3      0.78967     0.791126     0.789266     0.786787      0.788519  \n","4     0.786584     0.788698     0.781715     0.785351      0.787932  "]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["temp_file = -1\n","ks=['2', '3', '4', '5', '6', '7', '8', '9', '10']\n","\n","for df in dfs:\n","    temp_file += 1\n","    # loop through each k\n","    for k in ks:\n","        cur = 'gpt3_word_k' + k \n","        print('Coherence k ', k)\n","        # loop through each response\n","        for i in df.index:\n","            if type(df['gpt3_embed'][i]) != float:\n","                temp = []\n","                # calcuate similarity of word pairs at k inter-token distance\n","                for id,v in enumerate(df['gpt3_embed'][i]):\n","                    w1 = v\n","                    try:\n","                        w2 = df['gpt3_embed'][i][id + int(k)]\n","                    except IndexError:\n","                        continue\n","                    sim = cosine_similarity(w1, w2)\n","                    temp.append(sim) # a list of similarity scores for that response\n","                    #print('flag: ', temp)\n","                df[cur][i] = np.average(temp) # add other stats here\n","    df.to_csv(result + str(temp_file) + '.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Clinical factor scores"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>grid</th>\n","      <th>SSDvHC</th>\n","      <th>group</th>\n","      <th>tlc_01povspeech</th>\n","      <th>tlc_02povcontent</th>\n","      <th>tlc_03pressure</th>\n","      <th>tlc_04distract</th>\n","      <th>tlc_05tangent</th>\n","      <th>tlc_06derail</th>\n","      <th>tlc_07incoh</th>\n","      <th>...</th>\n","      <th>tlc_12circum</th>\n","      <th>tlc_13lossgoal</th>\n","      <th>tlc_14persev</th>\n","      <th>tlc_15echo</th>\n","      <th>tlc_16block</th>\n","      <th>tlc_17stilt</th>\n","      <th>tlc_18selfref</th>\n","      <th>tlc_3f_inefficient</th>\n","      <th>tlc_3f_incoherent</th>\n","      <th>tlc_3f_impexpress</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>10308</td>\n","      <td>1.0</td>\n","      <td>SSD</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>-0.406404</td>\n","      <td>-0.069358</td>\n","      <td>-0.018896</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10311</td>\n","      <td>0.0</td>\n","      <td>HC</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-0.713934</td>\n","      <td>-0.463481</td>\n","      <td>0.197262</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>10316</td>\n","      <td>1.0</td>\n","      <td>SSD</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-0.687947</td>\n","      <td>-0.387319</td>\n","      <td>-0.710348</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10455</td>\n","      <td>0.0</td>\n","      <td>HC</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-0.687947</td>\n","      <td>-0.387319</td>\n","      <td>-0.710348</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>10582</td>\n","      <td>1.0</td>\n","      <td>SSD</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-0.059923</td>\n","      <td>-0.107333</td>\n","      <td>-0.458561</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 24 columns</p>\n","</div>"],"text/plain":["    grid  SSDvHC group  tlc_01povspeech  tlc_02povcontent  tlc_03pressure  \\\n","1  10308     1.0   SSD                0                 1               0   \n","2  10311     0.0    HC                1                 0               0   \n","3  10316     1.0   SSD                0                 0               0   \n","4  10455     0.0    HC                0                 0               0   \n","5  10582     1.0   SSD                0                 0               0   \n","\n","   tlc_04distract  tlc_05tangent  tlc_06derail  tlc_07incoh  ...  \\\n","1               0              0             0            0  ...   \n","2               0              0             0            0  ...   \n","3               0              0             0            0  ...   \n","4               0              0             0            0  ...   \n","5               1              0             1            0  ...   \n","\n","   tlc_12circum  tlc_13lossgoal  tlc_14persev  tlc_15echo  tlc_16block  \\\n","1             0               0             1           0            0   \n","2             0               0             0           0            0   \n","3             0               0             0           0            0   \n","4             0               0             0           0            0   \n","5             1               1             0           0            0   \n","\n","   tlc_17stilt  tlc_18selfref  tlc_3f_inefficient  tlc_3f_incoherent  \\\n","1            2              0           -0.406404          -0.069358   \n","2            0              0           -0.713934          -0.463481   \n","3            0              0           -0.687947          -0.387319   \n","4            0              0           -0.687947          -0.387319   \n","5            0              0           -0.059923          -0.107333   \n","\n","   tlc_3f_impexpress  \n","1          -0.018896  \n","2           0.197262  \n","3          -0.710348  \n","4          -0.710348  \n","5          -0.458561  \n","\n","[5 rows x 24 columns]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["tlc = pd.read_csv('/Users/yancong/Desktop/4 clinical/00 Project Files/crossdx_clin.csv', index_col=0)\n","tlc = tlc[['grid', 'SSDvHC', 'group', 'tlc_01povspeech', 'tlc_02povcontent',\t'tlc_03pressure',\t'tlc_04distract',\n","\t'tlc_05tangent', 'tlc_06derail', 'tlc_07incoh',\t'tlc_08illogic',\t'tlc_09clang',\t'tlc_10neologism',\n","    \t'tlc_11wordapprox',\t'tlc_12circum',\t'tlc_13lossgoal',\t'tlc_14persev',\t'tlc_15echo',\t'tlc_16block',\n","        \t'tlc_17stilt',\t'tlc_18selfref', 'tlc_3f_inefficient',\t'tlc_3f_incoherent',\t'tlc_3f_impexpress']]\n","tlc.head()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/xl/6x4fjht11dq2blkl91wqmnkh0000gn/T/ipykernel_2423/1643671373.py:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['grid'] = df['grid'].astype(str)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>grid</th>\n","      <th>gpt3_word_k2</th>\n","      <th>gpt3_word_k3</th>\n","      <th>gpt3_word_k4</th>\n","      <th>gpt3_word_k5</th>\n","      <th>gpt3_word_k6</th>\n","      <th>gpt3_word_k7</th>\n","      <th>gpt3_word_k8</th>\n","      <th>gpt3_word_k9</th>\n","      <th>gpt3_word_k10</th>\n","      <th>...</th>\n","      <th>tlc_12circum</th>\n","      <th>tlc_13lossgoal</th>\n","      <th>tlc_14persev</th>\n","      <th>tlc_15echo</th>\n","      <th>tlc_16block</th>\n","      <th>tlc_17stilt</th>\n","      <th>tlc_18selfref</th>\n","      <th>tlc_3f_inefficient</th>\n","      <th>tlc_3f_incoherent</th>\n","      <th>tlc_3f_impexpress</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10455</td>\n","      <td>0.782686</td>\n","      <td>0.784116</td>\n","      <td>0.781053</td>\n","      <td>0.782103</td>\n","      <td>0.780552</td>\n","      <td>0.784993</td>\n","      <td>0.778847</td>\n","      <td>0.779661</td>\n","      <td>0.777854</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-0.687947</td>\n","      <td>-0.387319</td>\n","      <td>-0.710348</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11689</td>\n","      <td>0.789297</td>\n","      <td>0.78867</td>\n","      <td>0.779366</td>\n","      <td>0.784245</td>\n","      <td>0.785857</td>\n","      <td>0.786791</td>\n","      <td>0.782155</td>\n","      <td>0.786047</td>\n","      <td>0.78308</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-0.687947</td>\n","      <td>-0.387319</td>\n","      <td>-0.710348</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>12376</td>\n","      <td>0.789207</td>\n","      <td>0.785062</td>\n","      <td>0.785558</td>\n","      <td>0.788664</td>\n","      <td>0.78434</td>\n","      <td>0.790977</td>\n","      <td>0.785502</td>\n","      <td>0.791028</td>\n","      <td>0.783467</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-0.687947</td>\n","      <td>-0.387319</td>\n","      <td>-0.710348</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12630</td>\n","      <td>0.791977</td>\n","      <td>0.790015</td>\n","      <td>0.789015</td>\n","      <td>0.788958</td>\n","      <td>0.78967</td>\n","      <td>0.791126</td>\n","      <td>0.789266</td>\n","      <td>0.786787</td>\n","      <td>0.788519</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-0.687947</td>\n","      <td>-0.387319</td>\n","      <td>-0.710348</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>13493</td>\n","      <td>0.785416</td>\n","      <td>0.795869</td>\n","      <td>0.78943</td>\n","      <td>0.786664</td>\n","      <td>0.786584</td>\n","      <td>0.788698</td>\n","      <td>0.781715</td>\n","      <td>0.785351</td>\n","      <td>0.787932</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-0.687947</td>\n","      <td>-0.387319</td>\n","      <td>-0.710348</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 35 columns</p>\n","</div>"],"text/plain":["    grid gpt3_word_k2 gpt3_word_k3 gpt3_word_k4 gpt3_word_k5 gpt3_word_k6  \\\n","0  10455     0.782686     0.784116     0.781053     0.782103     0.780552   \n","1  11689     0.789297      0.78867     0.779366     0.784245     0.785857   \n","2  12376     0.789207     0.785062     0.785558     0.788664      0.78434   \n","3  12630     0.791977     0.790015     0.789015     0.788958      0.78967   \n","4  13493     0.785416     0.795869      0.78943     0.786664     0.786584   \n","\n","  gpt3_word_k7 gpt3_word_k8 gpt3_word_k9 gpt3_word_k10  ... tlc_12circum  \\\n","0     0.784993     0.778847     0.779661      0.777854  ...            0   \n","1     0.786791     0.782155     0.786047       0.78308  ...            0   \n","2     0.790977     0.785502     0.791028      0.783467  ...            0   \n","3     0.791126     0.789266     0.786787      0.788519  ...            0   \n","4     0.788698     0.781715     0.785351      0.787932  ...            0   \n","\n","  tlc_13lossgoal  tlc_14persev tlc_15echo  tlc_16block  tlc_17stilt  \\\n","0              0             0          0            0            0   \n","1              0             0          0            0            0   \n","2              0             0          0            0            0   \n","3              0             0          0            0            0   \n","4              0             0          0            0            0   \n","\n","   tlc_18selfref  tlc_3f_inefficient  tlc_3f_incoherent  tlc_3f_impexpress  \n","0              0           -0.687947          -0.387319          -0.710348  \n","1              0           -0.687947          -0.387319          -0.710348  \n","2              0           -0.687947          -0.387319          -0.710348  \n","3              0           -0.687947          -0.387319          -0.710348  \n","4              0           -0.687947          -0.387319          -0.710348  \n","\n","[5 rows x 35 columns]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["temp = -1\n","for df in dfs:\n","    temp += 1\n","    df = df[['grid',\n","       'gpt3_word_k2', 'gpt3_word_k3', 'gpt3_word_k4',\n","       'gpt3_word_k5', 'gpt3_word_k6', 'gpt3_word_k7', 'gpt3_word_k8',\n","       'gpt3_word_k9', 'gpt3_word_k10', 'gpt3_word_mv5', 'gpt3_word_mv10']]\n","    df['grid'] = df['grid'].astype(str)\n","    df = df.merge(tlc, on=['grid'])\n","    df.to_csv(result + str(temp) + '_GT.csv')\n","df.head()"]}],"metadata":{"colab":{"name":"gpt3_wordEmbd_foc_soc.ipynb","provenance":[],"toc_visible":true},"interpreter":{"hash":"c9bbc2a1c928760b171406dd0b75aa2bc786a11b1c33c4ac331981aff1f6a975"},"kernelspec":{"display_name":"Python 3.9.7 ('nlum1')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
