{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('sentence_clean.csv', index_col=0)\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp.grid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd = temp[temp.group.isin(['SSD'])]\n",
    "len(ssd.grid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = temp[temp.group.isin(['HC'])]\n",
    "len(hc.grid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from openai.embeddings_utils import get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = 'here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word - Add embeddings to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data and similarity comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the data from a saved file, you can run the following:\n",
    "\n",
    "df = pd.read_csv('sentence_clean.csv', index_col=0)\n",
    "df['babbage_similarity'] = df.babbage_similarity.apply(eval).apply(np.array)\n",
    "df['babbage_search'] = df.babbage_search.apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embedding representation is very rich and information dense.\n",
    "matrix = np.vstack(df.babbage_similarity.values)\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## traditional way get word embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subset dataframe calculations, change here\n",
    "# new directory\n",
    "df_full = pd.read_csv('word_level_deident.csv', index_col=0)\n",
    "df_full.speaker.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.task.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskin = ['AboutYourself', 'HowsItGoing', 'HowsItGoingx', ' HowsItGoing']\n",
    "df_full = df_full[df_full.task.isin(taskin)]\n",
    "df_full = df_full[df_full.speaker == \"Subject\"]\n",
    "df_word = df_full\n",
    "df_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gpt3(r):\n",
    "  if r[\"n_words\"] == 1 and r[\"is_partial\"] == 0 and r[\"is_speech_pause\"] == 0 and r['is_punctuation'] == 0 and r['is_stopword'] == 0:\n",
    "    return get_embedding(r['word_lower'], engine = 'text-similarity-babbage-001')\n",
    "  else:\n",
    "    return np.nan\n",
    "\n",
    "df_full[\"gpt3_embed\"] = df_full.apply(lambda r: calc_gpt3(r), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter word level dataframe \n",
    "df_word = df_full\n",
    "df_coh_word = df_word.loc[(df_word.is_speech_pause == 0) & (df_word.is_partial == 0) & (df_word.is_punctuation == 0) & (df_word.is_stopword == 0)]\n",
    "df_coh_word = df_coh_word[[\"uid\", \"task\", \"word_lower\", \"sentence_id\", \"seg_id\", \"content\", \"sp.tokenized\", \"sp.lemma\", \"gpt3_embed\"]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level: add min, max, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('sentence_clean.csv', index_col=0)\n",
    "gt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_embedding_of_sentence(sentence_embeddings):\n",
    "  print(sentence_embeddings.shape)\n",
    "  if sentence_embeddings.shape[0] > 0:\n",
    "    return np.average(sentence_embeddings.astype(np.float),0)\n",
    "  else:\n",
    "    return np.NaN\n",
    "\n",
    "def calc_response_cosine_similarity(sentence_embeddings):\n",
    "  from sklearn.metrics.pairwise import cosine_similarity\n",
    "  sim_matrix = cosine_similarity(sentence_embeddings)\n",
    "  resp_sim = 0\n",
    "  for i in range(sentence_embeddings.shape[0]-1):\n",
    "    resp_sim += sim_matrix[i][i+1]\n",
    "  resp_sim /= sentence_embeddings.shape[0] - 1\n",
    "  return resp_sim\n",
    "\n",
    "document_group = \"uid\"\n",
    "term_col = \"word_lower\"\n",
    "\n",
    "def get_idf_stats(r, N):\n",
    "  d = {}\n",
    "  d['doc_list'] = r[document_group].unique()\n",
    "  d['doc_count'] =  len(r[document_group].unique())\n",
    "  d[\"idf\"] = N / d['doc_count']\n",
    "  return pd.Series(d, index=['doc_list', 'doc_count', \"idf\"])\n",
    "\n",
    "N = len(df_coh_word[document_group].unique())\n",
    "\n",
    "# document frequency\n",
    "word_idf = df_coh_word.groupby([term_col]).apply(lambda x: get_idf_stats(x, N))\n",
    "word_idf\n",
    "\n",
    "# term frequency per document\n",
    "word_tf = df_coh_word.groupby([document_group,term_col]).agg({term_col: 'count'}).groupby(level=0).apply(lambda x: x / float(x.sum())).rename(columns={term_col:\"tf\"})\n",
    "word_tf\n",
    "\n",
    "# tf(t, d) * log(idf)\n",
    "def get_td_idf(term, document):\n",
    "  tf = word_tf.loc[document, term][\"tf\"]\n",
    "  idf = word_idf.loc[term][\"idf\"]\n",
    "  return tf * np.log(idf)\n",
    "\n",
    "embeddings = [\"gpt3_embed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns type from list to numpy.array [only saved in memory]\n",
    "df_coh_word['gpt3_embed'] = df_coh_word['gpt3_embed'].apply(lambda x: np.array(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coh_word = df_coh_word.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping = [\"uid\", \"task\"] \n",
    "\n",
    "df_coh_group = pd.DataFrame(columns = grouping)\n",
    "\n",
    "for idx, uid_df in df_coh_word.groupby(grouping):\n",
    "  embed_results = {}\n",
    "\n",
    "  # loop through all embeddings\n",
    "  for embed in embeddings:\n",
    "    turn_coherences_mean = []\n",
    "    turn_coherences_tf_idf = []\n",
    "\n",
    "    # create sentence embeddings\n",
    "    for idx2, sent_df in uid_df.groupby([\"sentence_id\"]):\n",
    "      embedd_array = []\n",
    "      tf_idf_weights = []\n",
    "      for _,r in sent_df.iterrows():\n",
    "        #manually converting back the entries to float (were string in df)\n",
    "        if type(r[embed]) != float and r[embed].shape[0] > 0:\n",
    "          embedd_array.append(r[embed])\n",
    "          tf_idf = get_td_idf(document = r[document_group], term = r[term_col])\n",
    "          tf_idf_weights.append(tf_idf)\n",
    "      # no embeddings for sentence\n",
    "      if len(embedd_array) < 1:\n",
    "        continue\n",
    "\n",
    "      # calc mean sentence:\n",
    "      mean_embed = np.average(embedd_array,0)\n",
    "      tf_idf_mean_embed = np.average(embedd_array, weights = tf_idf_weights, axis = 0)\n",
    "      \n",
    "      # save results for LSA back to array\n",
    "      if type(mean_embed) == np.ndarray:\n",
    "        turn_coherences_mean.append(mean_embed)\n",
    "      # else:\n",
    "      #   print(type(mean_embed))\n",
    "      if type(tf_idf_mean_embed) == np.ndarray:\n",
    "        turn_coherences_tf_idf.append(tf_idf_mean_embed)\n",
    "      # else:\n",
    "      #   print(type(tf_idf_mean_embed))\n",
    "\n",
    "    # end loop trhough sentences\n",
    "    # calculate cosine distance for the grouped embedding\n",
    "    if len(turn_coherences_mean) > 0:\n",
    "      turn_coherences_mean = np.stack( turn_coherences_mean, axis=0 ) # to np array\n",
    "      if turn_coherences_mean.shape[0] > 1:\n",
    "        mean_coh = calc_response_cosine_similarity(turn_coherences_mean)\n",
    "      else:\n",
    "        mean_coh = np.nan\n",
    "    else:\n",
    "      mean_coh = np.nan\n",
    "\n",
    "    embed_results[\"mean_\" + embed] = mean_coh\n",
    "\n",
    "    # calculate cosine distance for the grouped embedding\n",
    "    if len(turn_coherences_tf_idf) > 0:\n",
    "      turn_coherences_tf_idf = np.stack( turn_coherences_tf_idf, axis=0 ) # to np array\n",
    "      if turn_coherences_tf_idf.shape[0] > 1:\n",
    "        tf_idf_coh = calc_response_cosine_similarity(turn_coherences_tf_idf)\n",
    "      else:\n",
    "        tf_idf_coh = np.nan\n",
    "    else:\n",
    "      tf_idf_coh = np.nan\n",
    "\n",
    "    embed_results[\"tf_idf_\" + embed] = tf_idf_coh\n",
    "\n",
    "  # little nugget to depending on the grouping create the final DF of results\n",
    "  if(len(grouping) > 1):\n",
    "    i = 0\n",
    "    for g in grouping:\n",
    "      embed_results[g] = idx[i]\n",
    "      i = i + 1\n",
    "  else:\n",
    "    embed_results[grouping[0]] = idx\n",
    "\n",
    "  # write final results\n",
    "  df_coh_group = df_coh_group.append(pd.Series(embed_results), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min, max, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coh_word = df_full\n",
    "df_coh_word = df_word.loc[(df_word.is_speech_pause == 0) & (df_word.is_partial == 0) & (df_word.is_punctuation == 0) & (df_word.is_stopword == 0)]\n",
    "df_coh_word = df_coh_word[[\"uid\", \"task\", \"word_lower\", \"sentence_id\", \"seg_id\", \"content\", \"sp.tokenized\", \"sp.lemma\", \"gpt3_embed\"]] \n",
    "df_coh_word = df_coh_word.reset_index()\n",
    "df_coh_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coh_word = df_coh_word.loc[(df_coh_word.time == 'BL')]\n",
    "len(df_coh_word.uid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you load csv with embeddings, instead of use source dataframe, you need to\n",
    "# convert columns type from list to numpy.array, \n",
    "def convert(item):\n",
    "    item = str(item).strip()  # remove spaces at the end\n",
    "    item = str(item)[1:-1]    # remove `[ ]`\n",
    "    item = np.fromstring(item, sep=',')  # convert string to `numpy.array`\n",
    "    return item\n",
    "df_coh_word['gpt3_embed'] = df_coh_word['gpt3_embed'].apply(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coh_word['gpt3_embed'][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Array Dimension = \",len(df_coh_word['gpt3_embed'][22].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coh_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN\n",
    "grouping = [\"uid\", \"task\"] \n",
    "\n",
    "df_coh_group = pd.DataFrame(columns = grouping)\n",
    "\n",
    "for idx, uid_df in df_coh_word.groupby(grouping):\n",
    "  embed_results = {}\n",
    "\n",
    "  # loop through all embeddings\n",
    "  for embed in embeddings:\n",
    "    turn_coherences_min = []\n",
    "\n",
    "    # create sentence embeddings\n",
    "    for idx2, sent_df in uid_df.groupby([\"sentence_id\"]):\n",
    "      embedd_array = []\n",
    "      for i in sent_df.index:\n",
    "\n",
    "        try:\n",
    "          if type(sent_df[embed][i]) != 'float' and (sent_df[embed][i]).shape[0] > 0: # for aces and lpop\n",
    "            embedd_array.append(sent_df[embed][i])\n",
    "            tf_idf = get_td_idf(document = sent_df[document_group][i], term = sent_df[term_col][i])\n",
    "            tf_idf_weights.append(tf_idf)\n",
    "        except AttributeError:\n",
    "          continue\n",
    "\n",
    "      # no embeddings for sentence\n",
    "      if len(embedd_array) < 1:\n",
    "        continue\n",
    "   \n",
    "      # calc min sentence:\n",
    "      min_embed = np.min(np.array(embedd_array).astype(float),0) # \n",
    "\n",
    "      # save results for LSA back to array\n",
    "      if type(min_embed) == np.ndarray:\n",
    "        turn_coherences_min.append(min_embed)\n",
    "\n",
    "    # end loop trhough sentences\n",
    "    # calculate cosine distance for the grouped embedding\n",
    "    if len(turn_coherences_min) > 0:\n",
    "      turn_coherences_min = np.stack( turn_coherences_min, axis=0 ) # to np array\n",
    "      if turn_coherences_min.shape[0] > 1:\n",
    "        min_coh = calc_response_cosine_similarity(turn_coherences_min)\n",
    "      else:\n",
    "        min_coh = np.nan\n",
    "    else:\n",
    "      min_coh = np.nan\n",
    "\n",
    "    embed_results[\"min_\" + embed] = min_coh\n",
    "\n",
    "  # little nugget to depending on the grouping create the final DF of results\n",
    "  if(len(grouping) > 1):\n",
    "    i = 0\n",
    "    for g in grouping:\n",
    "      embed_results[g] = idx[i]\n",
    "      i = i + 1\n",
    "  else:\n",
    "    embed_results[grouping[0]] = idx\n",
    "\n",
    "  # write final results\n",
    "  df_coh_group = df_coh_group.append(pd.Series(embed_results), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coh_group_min = df_coh_group\n",
    "df_coh_group_min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX\n",
    "\n",
    "grouping = [\"uid\", \"task\"]\n",
    "\n",
    "df_coh_group = pd.DataFrame(columns = grouping)\n",
    "\n",
    "for idx, uid_df in df_coh_word.groupby(grouping):\n",
    "  embed_results = {}\n",
    "\n",
    "  # loop through all embeddings\n",
    "  for embed in embeddings:\n",
    "    turn_coherences_max = []\n",
    "\n",
    "    # create sentence embeddings\n",
    "    for idx2, sent_df in uid_df.groupby([\"sentence_id\"]):\n",
    "      embedd_array = []\n",
    "      for i in sent_df.index:\n",
    "        try:\n",
    "          if type(sent_df[embed][i]) != 'float' and (sent_df[embed][i]).shape[0] > 0: \n",
    "            embedd_array.append(sent_df[embed][i])\n",
    "        except AttributeError:\n",
    "          continue\n",
    "\n",
    "      # no embeddings for sentence\n",
    "      if len(embedd_array) < 1:\n",
    "        continue\n",
    "   \n",
    "      # calc max sentence:\n",
    "      max_embed = np.max(np.array(embedd_array).astype(float),0) # \n",
    "\n",
    "      # save results for LSA back to array\n",
    "      if type(max_embed) == np.ndarray:\n",
    "        turn_coherences_max.append(max_embed)\n",
    "\n",
    "    # end loop trhough sentences\n",
    "    # calculate cosine distance for the grouped embedding\n",
    "    if len(turn_coherences_max) > 0:\n",
    "      turn_coherences_max = np.stack( turn_coherences_max, axis=0 ) # to np array\n",
    "      if turn_coherences_max.shape[0] > 1:\n",
    "        max_coh = calc_response_cosine_similarity(turn_coherences_max)\n",
    "      else:\n",
    "        max_coh = np.nan\n",
    "    else:\n",
    "      max_coh = np.nan\n",
    "\n",
    "    embed_results[\"max_\" + embed] = max_coh\n",
    "\n",
    "  # little nugget to depending on the grouping create the final DF of results\n",
    "  if(len(grouping) > 1):\n",
    "    i = 0\n",
    "    for g in grouping:\n",
    "      embed_results[g] = idx[i]\n",
    "      i = i + 1\n",
    "  else:\n",
    "    embed_results[grouping[0]] = idx\n",
    "\n",
    "  # write final results\n",
    "  df_coh_group = df_coh_group.append(pd.Series(embed_results), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coh_group_max = df_coh_group\n",
    "df_coh_group_max.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD\n",
    "\n",
    "grouping = [\"uid\", \"task\"] \n",
    "\n",
    "df_coh_group = pd.DataFrame(columns = grouping)\n",
    "\n",
    "for idx, uid_df in df_coh_word.groupby(grouping):\n",
    "  embed_results = {}\n",
    "\n",
    "  # loop through all embeddings\n",
    "  for embed in embeddings:\n",
    "    turn_coherences_sd = []\n",
    "\n",
    "    # create sentence embeddings\n",
    "    for idx2, sent_df in uid_df.groupby([\"sentence_id\"]):\n",
    "      embedd_array = []\n",
    "      for i in sent_df.index:\n",
    "        try:\n",
    "          if type(sent_df[embed][i]) != 'float' and (sent_df[embed][i]).shape[0] > 0: # for aces and lpop\n",
    "            embedd_array.append(sent_df[embed][i])\n",
    "        except AttributeError:\n",
    "          continue\n",
    "\n",
    "      # no embeddings for sentence\n",
    "      if len(embedd_array) < 1:\n",
    "        continue\n",
    "   \n",
    "      # calc std sentence:\n",
    "      std_embed = np.std(np.array(embedd_array).astype(float),0) # \n",
    "\n",
    "      # save results for LSA back to array\n",
    "      if type(std_embed) == np.ndarray:\n",
    "        turn_coherences_sd.append(std_embed)\n",
    "  \n",
    "\n",
    "    # end loop trhough sentences\n",
    "    # calculate cosine distance for the grouped embedding\n",
    "    if len(turn_coherences_sd) > 0:\n",
    "      turn_coherences_sd = np.stack( turn_coherences_sd, axis=0 ) # to np array\n",
    "      if turn_coherences_sd.shape[0] > 1:\n",
    "        std_coh = calc_response_cosine_similarity(turn_coherences_sd)\n",
    "      else:\n",
    "        std_coh = np.nan\n",
    "    else:\n",
    "      std_coh = np.nan\n",
    "\n",
    "    embed_results[\"sd_\" + embed] = std_coh\n",
    "\n",
    "  # little nugget to depending on the grouping create the final DF of results\n",
    "  if(len(grouping) > 1):\n",
    "    i = 0\n",
    "    for g in grouping:\n",
    "      embed_results[g] = idx[i]\n",
    "      i = i + 1\n",
    "  else:\n",
    "    embed_results[grouping[0]] = idx\n",
    "\n",
    "  # write final results\n",
    "  df_coh_group = df_coh_group.append(pd.Series(embed_results), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coh_group_sd = df_coh_group\n",
    "df_coh_group_sd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge with gt and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.merge(df_coh_group_min, df_coh_group_max, how='left', on=['uid', 'task'])\n",
    "final = final.merge(df_coh_group_sd, how = 'left', on = ['uid', 'task'])\n",
    "final = final.merge(gt, how = 'left', on = ['uid', 'task'])\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final.uid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine with baseline embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = pd.read_csv('sentence_clean.csv', index_col=0)\n",
    "baseline = baseline[baseline.task.isin(['HowsItGoing', 'AboutYourself'])]\n",
    "baseline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add min, max, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = pd.read_csv('word_level_deindent.csv', index_col=0)\n",
    "source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(source.uid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_emb = source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter word level dataframe \n",
    "baseline_emb_word = baseline_emb\n",
    "baseline_coh_word = baseline_emb_word.loc[(baseline_emb_word.is_speech_pause == 0) & (baseline_emb_word.is_partial == 0) & (baseline_emb_word.is_punctuation == 0) & (baseline_emb_word.is_stopword == 0)]\n",
    "baseline_coh_word = baseline_coh_word[[\"uid\", \"grid\", \"time\", \"speaker\", \"task\", \"word_lower\", \"sentence_id\", \"seg_id\", \"content\", \"sp.tokenized\", \"sp.lemma\", \"lsa_embed\",\"glove_embed\", \"w2v_embed\"]] \n",
    "baseline_coh_word = baseline_coh_word[baseline_coh_word.speaker == 'Subject']\n",
    "baseline_coh_word = baseline_coh_word[baseline_coh_word.time == 'BL']\n",
    "baseline_coh_word = baseline_coh_word[baseline_coh_word.task.isin(['AboutYourself', 'HowsItGoing'])]\n",
    "\n",
    "baseline_coh_word = baseline_coh_word.reset_index()\n",
    "baseline_coh_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(baseline_coh_word.uid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_coh_word['lsa_embed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_coh_word['glove_embed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_coh_word['w2v_embed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coh_word = baseline_coh_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lsa(item):\n",
    "    item = str(item).strip()  # remove spaces at the end\n",
    "    item = str(item)[1:-1]    # remove `[ ]`\n",
    "    item = ' '.join(x.strip(\"'\") for x in item.split(' '))\n",
    "    item = np.fromstring(item, sep=' ')  # convert string to `numpy.array`\n",
    "    return item\n",
    "\n",
    "df_coh_word['lsa_embed'] = df_coh_word['lsa_embed'].apply(convert_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coh_word['lsa_embed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_glove_w2v(item):\n",
    "    item = str(item).strip()  # remove spaces at the end\n",
    "    item = str(item)[1:-1]    # remove `[ ]`\n",
    "    item = np.fromstring(item, sep=' ')  # convert string to `numpy.array`\n",
    "    return item\n",
    "\n",
    "df_coh_word['glove_embed'] = df_coh_word['glove_embed'].apply(convert_glove_w2v)\n",
    "df_coh_word['w2v_embed'] = df_coh_word['w2v_embed'].apply(convert_glove_w2v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_embedding_of_sentence(sentence_embeddings):\n",
    "  print(sentence_embeddings.shape)\n",
    "  if sentence_embeddings.shape[0] > 0:\n",
    "    return np.average(sentence_embeddings.astype(np.float),0)\n",
    "  else:\n",
    "    return np.NaN\n",
    "\n",
    "def calc_response_cosine_similarity(sentence_embeddings):\n",
    "  from sklearn.metrics.pairwise import cosine_similarity\n",
    "  sim_matrix = cosine_similarity(sentence_embeddings)\n",
    "  resp_sim = 0\n",
    "  for i in range(sentence_embeddings.shape[0]-1):\n",
    "    resp_sim += sim_matrix[i][i+1]\n",
    "  resp_sim /= sentence_embeddings.shape[0] - 1\n",
    "  return resp_sim\n",
    "\n",
    "document_group = \"uid\"\n",
    "term_col = \"word_lower\"\n",
    "\n",
    "def get_idf_stats(r, N):\n",
    "  d = {}\n",
    "  d['doc_list'] = r[document_group].unique()\n",
    "  d['doc_count'] =  len(r[document_group].unique())\n",
    "  d[\"idf\"] = N / d['doc_count']\n",
    "  return pd.Series(d, index=['doc_list', 'doc_count', \"idf\"])\n",
    "\n",
    "N = len(df_coh_word[document_group].unique())\n",
    "\n",
    "# document frequency\n",
    "word_idf = df_coh_word.groupby([term_col]).apply(lambda x: get_idf_stats(x, N))\n",
    "word_idf\n",
    "\n",
    "# term frequency per document\n",
    "word_tf = df_coh_word.groupby([document_group,term_col]).agg({term_col: 'count'}).groupby(level=0).apply(lambda x: x / float(x.sum())).rename(columns={term_col:\"tf\"})\n",
    "word_tf\n",
    "\n",
    "# tf(t, d) * log(idf)\n",
    "def get_td_idf(term, document):\n",
    "  tf = word_tf.loc[document, term][\"tf\"]\n",
    "  idf = word_idf.loc[term][\"idf\"]\n",
    "  return tf * np.log(idf)\n",
    "\n",
    "embeddings = ['lsa_embed', 'glove_embed', 'w2v_embed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping = [\"uid\", \"task\"] \n",
    "\n",
    "df_coh_group = pd.DataFrame(columns = grouping)\n",
    "\n",
    "for idx, uid_df in df_coh_word.groupby(grouping):\n",
    "  embed_results = {}\n",
    "\n",
    "  # loop through all embeddings\n",
    "  for embed in embeddings:\n",
    "    turn_coherences_min = []\n",
    "\n",
    "    # create sentence embeddings\n",
    "    for idx2, sent_df in uid_df.groupby([\"sentence_id\"]):\n",
    "      embedd_array = []\n",
    "      for i in sent_df.index:\n",
    "        try:\n",
    "          if type(sent_df[embed][i]) != 'float' and (sent_df[embed][i]).shape[0] > 0: # for aces and lpop\n",
    "            embedd_array.append(sent_df[embed][i])\n",
    "            tf_idf = get_td_idf(document = sent_df[document_group][i], term = sent_df[term_col][i])\n",
    "            tf_idf_weights.append(tf_idf)\n",
    "        except AttributeError:\n",
    "          continue\n",
    "\n",
    "      # no embeddings for sentence\n",
    "      if len(embedd_array) < 1:\n",
    "        continue\n",
    "   \n",
    "      # calc min sentence:\n",
    "      min_embed = np.min(np.array(embedd_array).astype(float),0) # \n",
    "\n",
    "      # save results for LSA back to array\n",
    "      if type(min_embed) == np.ndarray:\n",
    "        turn_coherences_min.append(min_embed)\n",
    "\n",
    "    # end loop trhough sentences\n",
    "    # calculate cosine distance for the grouped embedding\n",
    "    if len(turn_coherences_min) > 0:\n",
    "      turn_coherences_min = np.stack( turn_coherences_min, axis=0 ) # to np array\n",
    "      if turn_coherences_min.shape[0] > 1:\n",
    "        min_coh = calc_response_cosine_similarity(turn_coherences_min)\n",
    "      else:\n",
    "        min_coh = np.nan\n",
    "    else:\n",
    "      min_coh = np.nan\n",
    "\n",
    "    embed_results[\"min_\" + embed] = min_coh\n",
    "\n",
    "  # little nugget to depending on the grouping create the final DF of results\n",
    "  if(len(grouping) > 1):\n",
    "    i = 0\n",
    "    for g in grouping:\n",
    "      embed_results[g] = idx[i]\n",
    "      i = i + 1\n",
    "  else:\n",
    "    embed_results[grouping[0]] = idx\n",
    "\n",
    "  # write final results\n",
    "  df_coh_group = df_coh_group.append(pd.Series(embed_results), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df_coh_group_min = df_coh_group\n",
    "baseline_df_coh_group_min.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping = [\"uid\", \"task\"] \n",
    "\n",
    "df_coh_group = pd.DataFrame(columns = grouping)\n",
    "\n",
    "for idx, uid_df in df_coh_word.groupby(grouping):\n",
    "  embed_results = {}\n",
    "\n",
    "  # loop through all embeddings\n",
    "  for embed in embeddings:\n",
    "    turn_coherences_max = []\n",
    "\n",
    "    # create sentence embeddings\n",
    "    for idx2, sent_df in uid_df.groupby([\"sentence_id\"]):\n",
    "      embedd_array = []\n",
    "      for i in sent_df.index:\n",
    "        try:\n",
    "          if type(sent_df[embed][i]) != 'float' and (sent_df[embed][i]).shape[0] > 0: # for aces and lpop\n",
    "            embedd_array.append(sent_df[embed][i])\n",
    "        except AttributeError:\n",
    "          continue\n",
    "\n",
    "      # no embeddings for sentence\n",
    "      if len(embedd_array) < 1:\n",
    "        continue\n",
    "   \n",
    "      # calc max sentence:\n",
    "      max_embed = np.max(np.array(embedd_array).astype(float),0) # \n",
    "\n",
    "      # save results for LSA back to array\n",
    "      if type(max_embed) == np.ndarray:\n",
    "        turn_coherences_max.append(max_embed)\n",
    "\n",
    "    # end loop trhough sentences\n",
    "    # calculate cosine distance for the grouped embedding\n",
    "    if len(turn_coherences_max) > 0:\n",
    "      turn_coherences_max = np.stack( turn_coherences_max, axis=0 ) # to np array\n",
    "      if turn_coherences_max.shape[0] > 1:\n",
    "        max_coh = calc_response_cosine_similarity(turn_coherences_max)\n",
    "      else:\n",
    "        max_coh = np.nan\n",
    "    else:\n",
    "      max_coh = np.nan\n",
    "\n",
    "    embed_results[\"max_\" + embed] = max_coh\n",
    "\n",
    "  # little nugget to depending on the grouping create the final DF of results\n",
    "  if(len(grouping) > 1):\n",
    "    i = 0\n",
    "    for g in grouping:\n",
    "      embed_results[g] = idx[i]\n",
    "      i = i + 1\n",
    "  else:\n",
    "    embed_results[grouping[0]] = idx\n",
    "\n",
    "  # write final results\n",
    "  df_coh_group = df_coh_group.append(pd.Series(embed_results), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df_coh_group_max = df_coh_group\n",
    "baseline_df_coh_group_max.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD\n",
    "\n",
    "grouping = [\"uid\", \"task\"]\n",
    "\n",
    "df_coh_group = pd.DataFrame(columns = grouping)\n",
    "\n",
    "for idx, uid_df in df_coh_word.groupby(grouping):\n",
    "  embed_results = {}\n",
    "\n",
    "  # loop through all embeddings\n",
    "  for embed in embeddings:\n",
    "    turn_coherences_sd = []\n",
    "\n",
    "    # create sentence embeddings\n",
    "    for idx2, sent_df in uid_df.groupby([\"sentence_id\"]):\n",
    "      embedd_array = []\n",
    "      for i in sent_df.index:\n",
    "        \n",
    "        try:\n",
    "          if type(sent_df[embed][i]) != 'float' and (sent_df[embed][i]).shape[0] > 0: # for aces and lpop\n",
    "            embedd_array.append(sent_df[embed][i])\n",
    "        except AttributeError:\n",
    "          continue\n",
    "\n",
    "      # no embeddings for sentence\n",
    "      if len(embedd_array) < 1:\n",
    "        continue\n",
    "   \n",
    "      # calc std sentence:\n",
    "      std_embed = np.std(np.array(embedd_array).astype(float),0) # \n",
    "\n",
    "      # save results for LSA back to array\n",
    "      if type(std_embed) == np.ndarray:\n",
    "        turn_coherences_sd.append(std_embed)\n",
    "  \n",
    "\n",
    "    # end loop trhough sentences\n",
    "    # calculate cosine distance for the grouped embedding\n",
    "    if len(turn_coherences_sd) > 0:\n",
    "      turn_coherences_sd = np.stack( turn_coherences_sd, axis=0 ) # to np array\n",
    "      if turn_coherences_sd.shape[0] > 1:\n",
    "        std_coh = calc_response_cosine_similarity(turn_coherences_sd)\n",
    "      else:\n",
    "        std_coh = np.nan\n",
    "    else:\n",
    "      std_coh = np.nan\n",
    "\n",
    "    embed_results[\"sd_\" + embed] = std_coh\n",
    "\n",
    "  # little nugget to depending on the grouping create the final DF of results\n",
    "  if(len(grouping) > 1):\n",
    "    i = 0\n",
    "    for g in grouping:\n",
    "      embed_results[g] = idx[i]\n",
    "      i = i + 1\n",
    "  else:\n",
    "    embed_results[grouping[0]] = idx\n",
    "\n",
    "  # write final results\n",
    "  df_coh_group = df_coh_group.append(pd.Series(embed_results), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df_coh_group_sd = df_coh_group\n",
    "baseline_df_coh_group_sd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new merge and save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final.uid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalfinal = final.merge(baseline_df_coh_group_min, how='left', on=['uid', 'task'])\n",
    "finalfinal = finalfinal.merge(baseline_df_coh_group_max, how='left', on=['uid', 'task'])\n",
    "finalfinal = finalfinal.merge(baseline_df_coh_group_sd, how='left', on = ['uid', 'task'])\n",
    "finalfinal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(finalfinal.uid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical = pd.read_csv('features_table.csv', index_col=0)\n",
    "clinical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gt.uid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence similarity get_emb(sentence pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import csv\n",
    "import torch\n",
    "import scipy\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run through clean sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentence_clean.csv', index_col=0)\n",
    "df = df[df.task.isin(['HowsItGoing', 'AboutYourself'])]\n",
    "df = df.rename(columns={'roberta_similarity': 'gpt_similarity'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.task.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buglst = {}\n",
    "for i,r in df.iterrows():\n",
    "  if r['speaker'] != 'Interviewer':\n",
    "    try:\n",
    "      sentence1 = r['content']\n",
    "      sentence2 = df['content'][i+1]\n",
    "\n",
    "      # encode sentences to get their embeddings\n",
    "      embedding1 = get_embedding(sentence1, engine = 'text-similarity-babbage-001')\n",
    "      embedding2 = get_embedding(sentence2, engine = 'text-similarity-babbage-001')\n",
    "\n",
    "      # compute similarity scores of two embeddings\n",
    "      cosine_similarity = 1 - scipy.spatial.distance.cosine(embedding1, embedding2)\n",
    "      print(\"Sentence 1:\", sentence1)\n",
    "      print(\"Sentence 2:\", sentence2)\n",
    "      print(\"Similarity score:\", cosine_similarity)\n",
    "\n",
    "      df['gpt_similarity'][i+1] = cosine_similarity\n",
    "\n",
    "    except KeyError:\n",
    "      buglst[df['Unnamed: 0.1'][i]] = df['content'][i]\n",
    "      continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge word, sentence, and clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = pd.read_csv('data_analysis.csv', index_col=0)\n",
    "word = word[[\"grid\", \"task\",  \"group\",\n",
    "\"tlc_3f_psy\", \"tlc_3f_nonsp\", \"tlc_3f_negative\", \n",
    "'mean_gpt3_embed', 'mean_lsa_embed', 'mean_glove_embed', 'mean_w2v_embed',\n",
    "'min_gpt3_embed', 'min_lsa_embed', 'min_glove_embed', 'min_w2v_embed', \n",
    "'max_gpt3_embed', 'max_lsa_embed', 'max_glove_embed', 'max_w2v_embed', \n",
    "'sd_gpt3_embed', 'sd_lsa_embed', 'sd_glove_embed', 'sd_w2v_embed'\n",
    "]]\n",
    "word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word.grid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = pd.read_csv('/Users/yancong/Desktop/5 zili research/clinicalNLP/gpt3_embeddings/stats/remora_sentence_clean_similarity_gt.csv', index_col=0)\n",
    "sentence = sentence[[\"grid\", \"task\",  \"group\",\n",
    "\"tlc_3f_psy\", \"tlc_3f_nonsp\", \"tlc_3f_negative\", \n",
    "'similarity_mean_roberta', 'similarity_mean_t5', 'gpt_similarity_mean', \n",
    "'roberta_similarity_min', 't5small_similarity_min', 'gpt_similarity_min', \n",
    "'roberta_similarity_max', 't5small_similarity_max', 'gpt_similarity_max', \n",
    "'roberta_similarity_std', 't5small_similarity_std', 'gpt_similarity_std'\n",
    "]]\n",
    "sentence = sentence[sentence.task.isin(['AboutYourself', 'HowsItGoing'])]\n",
    "sentence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence.grid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = pd.merge(word, sentence, on=['grid', 'group', 'task', \"tlc_3f_psy\", \"tlc_3f_nonsp\", \"tlc_3f_negative\", ], how='left')\n",
    "fdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fdf.grid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.to_csv('data_analysis.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9bbc2a1c928760b171406dd0b75aa2bc786a11b1c33c4ac331981aff1f6a975"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlum1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
