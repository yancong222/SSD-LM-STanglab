{"cells":[{"cell_type":"markdown","metadata":{},"source":["# note: This program needs openAI API"]},{"cell_type":"markdown","metadata":{"id":"MXUX3Z1dJOcw"},"source":["# Prep dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZuHPbrbAJOcy"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import scipy\n","from torch import tensor\n","\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# reading in data by sample\n","# each sample is collapsed from the two open-ended prompts\n","\n","data_foloder = 'your folder'\n","\n","baseline = pd.read_csv(data_foloder + 'test.csv')\n","\n","baseline.head() "]},{"cell_type":"markdown","metadata":{},"source":["# Install lib and dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rmelhcu1JOc2","outputId":"31e9eed6-24ab-4a28-d4a2-0607f26f899e"},"outputs":[],"source":["# no need to install it every time you open it\n","# for GPT-3\n","# install GPT-3 from huggingface\n","!pip install transformers "]},{"cell_type":"code","execution_count":3,"metadata":{"id":"jf7X4aosJOc2"},"outputs":[],"source":["# import tokenizers from GPT2TokenizerFast (GPT-3 uses the same tokenizer)\n","\n","import transformers\n","from transformers import GPT2TokenizerFast\n","tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"-biYC5D0JOc5"},"outputs":[],"source":["# use openai API\n","from openai.embeddings_utils import get_embeddings, cosine_similarity # important\n","import openai\n","openai.api_key = 'your key'"]},{"cell_type":"markdown","metadata":{},"source":["# Get word embeddings from whole response embeddings GPT3 3df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["buglist = {} # initialize dic to log bugs\n","def calc_gpt3(r): # defining function for extracting contextualized embeddings from verbatim transcripts\n","  if r[0] % 2 != 0: # keeping track of progress - print out even grid as we process them\n","    print('current grid: ', r[0])\n","  if type(r['content']) == str and r['n_words'] < 2048: # if the content is string [non-empty]; 2048 is max for GPT-3\n","    temp = r['content'].split(' ') # gives a list of words\n","    input = ' '.join([i for i in temp if (i != '') & ('{' not in i) & ('}' not in i) & ('#' not in i)]) # exclude NSV and the restart symbol\n","    try:\n","      # take in the whole response as a list of words; 2048 dimensions\n","      # https://beta.openai.com/docs/guides/embeddings/what-are-embeddings\n","      vec = get_embeddings(input.split(' '), engine = 'your engine') \n","    except:\n","      buglist[r[0]] = input.split(' ') # store grid if there is a bug\n","    return vec\n","  else:\n","    return np.nan\n","\n","# double check your input df\n","# make sure there is no '  ' or '   '\n","\n","df = baseline\n","temp = 0\n","df = df[df['grid'] == 'test000'] # only select 1 individual for demo purpose\n","df[\"gpt3_embed\"] = '' # create new empty column\n","df[\"gpt3_embed\"] = df.apply(lambda r: calc_gpt3(r), axis = 1) # apply embedding function to df\n","df.to_csv(str(temp) + '.csv')\n","\n","df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["177"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["len(df['gpt3_embed'][1]) # the number of tokens in that response"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["2048"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# every token get a len (i.e. dimension) 2048 embedding vector\n","len(list(df['gpt3_embed'][1])[0]) "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["{}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["buglist"]},{"cell_type":"markdown","metadata":{},"source":["# MV 5/10"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['Alex', 'broke', 'the', 'vase', 'accidentally'], ['.', 'But', 'Kai', 'did', 'it'], ['on', 'purpose', '.']]\n"]}],"source":["# Average semantic similarity of each word in 5- or 10- words window\n","\n","def divide_chunks(l, n):\n","      \n","    # looping till length l\n","    for i in range(0, len(l), n): \n","        yield l[i:i + n]\n","  \n","# n: How many elements each\n","# list should have\n","test = ['Alex','broke','the','vase','accidentally','.','But','Kai','did','it','on','purpose','.']\n","divide_chunks(test,5)\n","chopped = list(divide_chunks(test,5))\n","print(chopped)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['Alex', 'broke'], ['Alex', 'the'], ['Alex', 'vase'], ['Alex', 'accidentally'], ['broke', 'the'], ['broke', 'vase'], ['broke', 'accidentally'], ['the', 'vase'], ['the', 'accidentally'], ['vase', 'accidentally']]\n"]}],"source":["def combinations(lst): # get w1, w2 combinations\n","    # input: a list of <= 5 tokens\n","    cmb = []\n","    rightside = lst[:] # initialize a list\n","    for wid, w1 in enumerate(lst): # each token gets a chance to be w1\n","        rightside = lst[wid:] # dynamically chop off w1 from the rest of the list\n","        while rightside: # loop until the rest of the list is empty\n","            w2 = rightside.pop(0) # stack up w2\n","            if w2 != w1: # get rid of ['Alex', 'Alex']\n","                cmb.append([w1, w2])  \n","    return cmb\n","\n","testing = ['Alex', 'broke', 'the', 'vase', 'accidentally']\n","result = combinations(testing)\n","print(result)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# stats ignoring nan\n","from numpy import nanmedian\n","\n","import scipy\n","def iqr(x):\n","  return scipy.stats.iqr(np.array(x), nan_policy='omit')\n","\n","from numpy import quantile\n","def q5(x):\n","    return np.quantile(np.array(x), 0.05)\n","\n","def q95(x):\n","    return np.quantile(np.array(x), 0.95)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["temp_file = 0 # give df a temp name; the intermediate df will be discarded bcs otherwise it occupies too much space\n","mvs=['5', '10']\n","stats = ['_median', '_iqr', '_q5', '_q95'] # appended to the embeddings df\n","dfs = [df]\n","for df in dfs:\n","    temp += 1\n","    # create new empty columns\n","    for mv in mvs:\n","        for stat in stats:\n","            cur = 'gpt3_word_mv' + mv + stat\n","            df[cur] = ''\n","            df.to_csv(str(temp) + '.csv')\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for mv in mvs:\n","    # print progress\n","    cur = 'gpt3_word_mv' + mv\n","    print('current: ', cur)\n","    df[cur + '_similarity'] = '' # ssave the cosine similarities; all stats are derived from there\n","\n","    # loop over each response\n","    for i in df.index:\n","        if type(df['gpt3_embed'][i]) != float: \n","            # chop 1 big response sequence into 5/10-token chunks\n","            word_embed_chunk = list(divide_chunks(df['gpt3_embed'][i], int(mv)))\n","            chunk_temp_collection = [] \n","            # loop over each 5/10 chunk in the response\n","            for chunck_id, word_embed in enumerate(word_embed_chunk):\n","                temp_collection = []\n","                # calculate average similarities for that chunk (5 or 10 window)\n","                cmbs = combinations(word_embed) # apply function \n","                for cmb in cmbs:\n","                    w1 = cmb[0]\n","                    w2 = cmb[1]\n","                    temp = cosine_similarity(w1, w2)\n","                    temp_collection.append(temp)\n","                temp_sim = np.nanmean(temp_collection)\n","                chunk_temp_collection.append(temp_sim) # incrementally append similarity mean to the list \n","\n","        # get a list of similarity means for that response, \n","        # its len is the number of chunks that the response can be chopped into\n","        df[cur + '_similarity'][i] = chunk_temp_collection # similarity mv 5 or 10; store it for later reference/stats\n","\n","        # add other stats here\n","        df[cur + '_median'][i] = np.nanmedian(chunk_temp_collection)\n","        df[cur + '_q5'][i] = q5(chunk_temp_collection)\n","        df[cur + '_q95'][i] = q95(chunk_temp_collection)\n","        df[cur + '_iqr'][i] = iqr(chunk_temp_collection)\n","        \n","df.to_csv(str(temp_file) + '.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# K1:10"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import ast # a module that evaluates mathematical expressions and statements\n","\n","temp = -1\n","ks=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n","stats = ['_median', '_iqr', '_q5', '_q95']\n","dfs = [df]\n","for df in dfs:\n","    temp += 1\n","    # create new empty columns\n","    for k in ks:\n","        for stat in stats:\n","            cur = 'gpt3_word_k' + k + stat\n","            df[cur] = ''\n","            df.to_csv(str(temp) + '.csv')\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["temp_file = -1\n","ks=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n","\n","for df in dfs:\n","    temp_file += 1\n","    # loop through each k\n","    for k in ks:\n","        cur = 'gpt3_word_k' + k \n","        print('Coherence k ', k) # progress\n","        df[cur + '_similarity'] = ''\n","        # loop through each individual's response \n","        for i in df.index:\n","            if type(df['gpt3_embed'][i]) != float:\n","                temp = []\n","                # calcuate similarity of word pairs at k inter-token distance\n","                for id,v in enumerate(df['gpt3_embed'][i]):\n","                    w1 = v\n","                    try:\n","                        w2 = df['gpt3_embed'][i][id + int(k)]\n","                    except IndexError:\n","                        continue\n","                    sim = cosine_similarity(w1, w2)\n","                    temp.append(sim) # a list of similarity scores for that response\n","\n","                # intermediate df, save \n","                df[cur + '_similarity'][i] = temp\n","                df[cur + '_iqr'][i] = iqr(temp) # add other stats here\n","                df[cur + '_median'][i] = np.nanmedian(temp)\n","                df[cur + '_q5'][i] = q5(temp)\n","                df[cur + '_q95'][i] = q95(temp)\n","    df.to_csv(str(temp_file) + '.csv')\n","df.head()"]}],"metadata":{"colab":{"name":"gpt3_wordEmbd_foc_soc.ipynb","provenance":[],"toc_visible":true},"interpreter":{"hash":"c9bbc2a1c928760b171406dd0b75aa2bc786a11b1c33c4ac331981aff1f6a975"},"kernelspec":{"display_name":"Python 3.9.7 ('nlum1')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
