{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import re\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'here'\n",
    "result = 'here'\n",
    "\n",
    "baseline = pd.read_csv(data_folder + 'here')\n",
    "baseline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ec7Q2J6vLLoE"
   },
   "source": [
    "# Install lib and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade gensim==3.8.3\n",
    "import string\n",
    "import math\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.models as gs\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbyAAHxdTFdR"
   },
   "source": [
    "# Get GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove embedding\n",
    "fname = get_tmpfile(\"here\")\n",
    "model = KeyedVectors.load_word2vec_format(fname)\n",
    "glove = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swords = stopwords.words('english')\n",
    "swords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [baseline, incoh10, incoh20, incoh50, ineff10, ineff20, ineff50]\n",
    "temp = -1\n",
    "for df in dfs:\n",
    "  temp += 1\n",
    "  for i in df.index:\n",
    "    df['content'][i] = df['content'][i].split(' ')\n",
    "  df = df.explode('content')\n",
    "  df['word_lower'] = df['content'].str.lower()\n",
    "  df[\"nltk.tokenized\"] = df.apply(lambda row: nltk.word_tokenize(str(row[\"word_lower\"]).lower()) , axis = 1)\n",
    "  df[\"nltk.n_token\"] = df.apply(lambda row: len(row[\"nltk.tokenized\"]) , axis = 1)\n",
    "  df[\"nltk.n_stopword\"] = df.apply(lambda row: len(list(filter(lambda x: x in swords, row[\"nltk.tokenized\"]))), axis = 1 )\n",
    "  df[\"is_stopword\"] = df.apply(lambda row: 1 if row[\"nltk.n_stopword\"] > 0 else 0, axis=1)\n",
    "  df.to_csv(result + str(temp) + '.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_glove(r):\n",
    "  if type(r[\"word_lower\"]) != float and r[\"word_lower\"] in glove:\n",
    "    return glove[r[\"word_lower\"]] \n",
    "  else:\n",
    "    return np.nan\n",
    "\n",
    "baseline = pd.read_csv(result + '0.csv', index_col=0)\n",
    "incoh10 = pd.read_csv(result + '1.csv', index_col=0)\n",
    "incoh20 = pd.read_csv(result + '2.csv', index_col=0)\n",
    "incoh50 = pd.read_csv(result + '3.csv', index_col=0)\n",
    "ineff10 = pd.read_csv(result + '4.csv', index_col=0)\n",
    "ineff20 = pd.read_csv(result + '5.csv', index_col=0)\n",
    "ineff50 = pd.read_csv(result + '6.csv', index_col=0)\n",
    "\n",
    "dfs = [baseline, incoh10, incoh20, incoh50, ineff10, ineff20, ineff50]\n",
    "\n",
    "temp = -1\n",
    "for df in dfs:\n",
    "  temp += 1\n",
    "  df[\"glove_emb\"] = ''\n",
    "  df[\"glove_emb\"] = df.apply(lambda r: calc_glove(r), axis = 1)\n",
    "  df.to_csv(result + str(temp) + '.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['glove_emb'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['glove_emb'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    df.dropna(subset = ['glove_emb'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group together for sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = -1\n",
    "for df in dfs:\n",
    "    temp += 1\n",
    "    if temp == 0:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        baseline = df\n",
    "    if temp == 1:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        incoh10 = df\n",
    "    if temp == 2:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        incoh20 = df\n",
    "    if temp == 3:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        incoh50 = df\n",
    "    if temp == 4:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        ineff10 = df\n",
    "    if temp ==5:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        ineff20 = df\n",
    "    if temp == 6:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        ineff50 = df\n",
    "\n",
    "dfs = [baseline, incoh10, incoh20, incoh50, ineff10, ineff20, ineff50]\n",
    "incoh20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['glove_emb'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['glove_emb'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.grid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats and similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats ignoring nan\n",
    "from numpy import nanmedian\n",
    "\n",
    "import scipy\n",
    "def iqr(x):\n",
    "  return scipy.stats.iqr(np.array(x), nan_policy='omit')\n",
    "\n",
    "from numpy import nanquantile\n",
    "def q5(x):\n",
    "    return np.nanquantile(np.array(x), 0.05)\n",
    "\n",
    "def q95(x):\n",
    "    return np.nanquantile(np.array(x), 0.95)\n",
    "\n",
    "# cosine_similarity, apply to all LMs\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MV 5/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Alex', 'broke', 'the', 'vase', 'accidentally'], ['.', 'But', 'Kai', 'did', 'it'], ['on', 'purpose', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Average semantic similarity of each word in 5- or 10- words window\n",
    "\n",
    "def divide_chunks(l, n):\n",
    "      \n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]\n",
    "  \n",
    "# n: How many elements each\n",
    "# list should have\n",
    "test = ['Alex','broke','the','vase','accidentally','.','But','Kai','did','it','on','purpose','.']\n",
    "divide_chunks(test,5)\n",
    "chopped = list(divide_chunks(test,5))\n",
    "print(chopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Alex', 'broke'], ['Alex', 'the'], ['Alex', 'vase'], ['Alex', 'accidentally'], ['broke', 'the'], ['broke', 'vase'], ['broke', 'accidentally'], ['the', 'vase'], ['the', 'accidentally'], ['vase', 'accidentally']]\n"
     ]
    }
   ],
   "source": [
    "def combinations(lst): # get w1, w2 combinations\n",
    "    # input: a list of <= 5 tokens\n",
    "    cmb = []\n",
    "    rightside = lst[:] # initialize a list\n",
    "    for wid, w1 in enumerate(lst): # each token gets a chance to be w1\n",
    "        rightside = lst[wid:] # dynamically chop off w1 from the rest of the list\n",
    "        while rightside: # loop until the rest of the list is empty\n",
    "            w2 = rightside.pop(0) # stack up w2\n",
    "            if w2 != w1: # get rid of ['Alex', 'Alex']\n",
    "                cmb.append([w1, w2])  \n",
    "    return cmb\n",
    "\n",
    "testing = ['Alex', 'broke', 'the', 'vase', 'accidentally']\n",
    "test_result = combinations(testing)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvs=['5', '10']\n",
    "stats = ['_median', '_iqr', '_q5', '_q95'] # appended to the embeddings df\n",
    "for df in dfs:\n",
    "    # create new empty columns\n",
    "    for mv in mvs:\n",
    "        for stat in stats:\n",
    "            cur = 'glove_mv' + mv + stat\n",
    "            df[cur] = ''\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['glove_emb'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = -1\n",
    "for df in dfs:\n",
    "    temp_file += 1\n",
    "    print(temp_file)\n",
    "    for mv in mvs:\n",
    "        # print progress\n",
    "        cur = 'glove_mv' + mv\n",
    "        print('current: ', cur)\n",
    "        df[cur + '_similarity'] = '' # save the cosine similarities; all stats are derived from there\n",
    "\n",
    "        # loop over each response\n",
    "        for i in df.index:\n",
    "            if type(df['glove_emb'][i]) != float: \n",
    "                # chop 1 big response sequence into 5/10-token chunks\n",
    "                word_embed_chunk = list(divide_chunks(df['glove_emb'][i].tolist(), int(mv))) # add .tolist() bcs glove emb gives numpy.ndarray # add [0] bcs glove emb shape (1, 385, 300)\n",
    "                chunk_temp_collection = [] \n",
    "                # loop over each 5/10 chunk in the response\n",
    "                for chunck_id, word_embed in enumerate(word_embed_chunk):\n",
    "                    temp_collection = []\n",
    "                    # calculate average similarities for that chunk (5 or 10 window)\n",
    "                    cmbs = combinations(word_embed) # apply function \n",
    "                    for cmb in cmbs:\n",
    "                        w1 = cmb[0]\n",
    "                        w2 = cmb[1]\n",
    "                        temp = cosine_similarity(w1, w2)\n",
    "                        temp_collection.append(temp)\n",
    "                    temp_sim = np.nanmean(temp_collection)\n",
    "                    chunk_temp_collection.append(temp_sim) # incrementally append similarity mean to the list \n",
    "\n",
    "            # get a list of similarity means for that response, \n",
    "            # its len is the number of chunks that the response can be chopped into\n",
    "            df[cur + '_similarity'][i] = chunk_temp_collection # similarity mv 5 or 10; store it for later reference/stats\n",
    "\n",
    "            # add other stats here\n",
    "            df[cur + '_median'][i] = np.nanmedian(chunk_temp_collection)\n",
    "            df[cur + '_q5'][i] = q5(chunk_temp_collection)\n",
    "            df[cur + '_q95'][i] = q95(chunk_temp_collection)\n",
    "            df[cur + '_iqr'][i] = iqr(chunk_temp_collection)\n",
    "    df.to_csv(result + str(temp_file) + '_GloVe.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K1:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast # a module that evaluates mathematical expressions and statements\n",
    "\n",
    "ks=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "stats = ['_median', '_iqr', '_q5', '_q95']\n",
    "for df in dfs:\n",
    "    # create new empty columns\n",
    "    for k in ks:\n",
    "        for stat in stats:\n",
    "            cur = 'glove_k' + k + stat\n",
    "            df[cur] = ''\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = -1\n",
    "ks=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "for df in dfs:\n",
    "    temp_file += 1\n",
    "    # loop through each k\n",
    "    for k in ks:\n",
    "        cur = 'glove_k' + k \n",
    "        print('Coherence k ', k) # progress\n",
    "        df[cur + '_similarity'] = ''\n",
    "        # loop through each individual's response \n",
    "        for i in df.index:\n",
    "            if type(df['glove_emb'][i]) != float:\n",
    "                temp = []\n",
    "                # calcuate similarity of word pairs at k inter-token distance\n",
    "                for id,v in enumerate(df['glove_emb'][i]): # add [0] bcs glove emb shape (1, 385, 300)\n",
    "                    w1 = v\n",
    "                    try:\n",
    "                        w2 = df['glove_emb'][i][id + int(k)]\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                    sim = cosine_similarity(w1, w2)\n",
    "                    temp.append(sim) # a list of similarity scores for that response\n",
    "\n",
    "                # intermediate df, save \n",
    "                df[cur + '_similarity'][i] = temp\n",
    "                df[cur + '_iqr'][i] = iqr(temp) # add other stats here\n",
    "                df[cur + '_median'][i] = np.nanmedian(temp)\n",
    "                df[cur + '_q5'][i] = q5(temp)\n",
    "                df[cur + '_q95'][i] = q95(temp)\n",
    "    df.to_csv(result + str(temp_file) + '_GloVe.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOC and SOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs=['foc', 'soc']\n",
    "stats = ['_median', '_iqr', '_q5', '_q95']\n",
    "for df in dfs:\n",
    "    # create new empty columns\n",
    "    for c in cs:\n",
    "        for stat in stats:\n",
    "            cur = 'glove_' + c + stat\n",
    "            df[cur] = ''\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = -1 # keep track of file names\n",
    "for df in dfs:\n",
    "    print('progress')\n",
    "    temp_file += 1\n",
    "    df['glove_foc_similarity'] = ''\n",
    "    df['glove_soc_similarity'] = ''\n",
    "    # loop over each response\n",
    "    for i in df.index:\n",
    "        temp_foc = [] # for each individual, store the list of cos similarity\n",
    "        temp_soc = []\n",
    "        if type(df['glove_emb'][i]) != float:\n",
    "            # calculate average similarity for sentence pairs, either adjacent or with one intervening\n",
    "            for idx, sent in enumerate(df['glove_emb'][i]):\n",
    "                try:\n",
    "                    temp_foc.append(cosine_similarity(sent, df['glove_emb'][i][idx+1])) # get a list of similarities for that response\n",
    "                    \n",
    "                    df['glove_foc_similarity'][i] = temp_foc # record intermediate similarities \n",
    "                    \n",
    "                    df['glove_foc_median'][i] = np.nanmedian(temp_foc) # add more stats here\n",
    "                    df['glove_foc_iqr'][i] = iqr(temp_foc)\n",
    "                    df['glove_foc_q5'][i] = q5(temp_foc)\n",
    "                    df['glove_foc_q95'][i] = q95(temp_foc)\n",
    "\n",
    "                    temp_soc.append(cosine_similarity(sent, df['glove_emb'][i][idx+2]))\n",
    "\n",
    "                    df['glove_soc_similarity'][i] = temp_soc # record intermediate similarities \n",
    "\n",
    "                    df['glove_soc_median'][i] = np.nanmedian(temp_soc)\n",
    "                    df['glove_soc_iqr'][i] = iqr(temp_soc)\n",
    "                    df['glove_soc_q5'][i] = q5(temp_soc)\n",
    "                    df['glove_soc_q95'][i] = q95(temp_soc)\n",
    "\n",
    "                except IndexError:\n",
    "                    continue\n",
    "    print('finished')\n",
    "    df.to_csv(result + str(temp_file) + '_Glove.csv')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rdfu-cu9_CEY"
   ],
   "machine_shape": "hm",
   "name": "ACES_basic_gt_processing.ipynb",
   "provenance": [
    {
     "file_id": "1ETF3x6oEJywFFsX0tCKRuVjBHgxh8Qrd",
     "timestamp": 1637704592379
    },
    {
     "file_id": "13oLsCUJOgMCcnuSIFhBO0gX9CGSJdTQB",
     "timestamp": 1637691259779
    },
    {
     "file_id": "1FQh9dMlL6_Wg-A5-kxq7oYMTF_cstmUE",
     "timestamp": 1637248901827
    },
    {
     "file_id": "1ogy3oiNfRRjrrZUlOsMUxo5CxKM-xR0o",
     "timestamp": 1617996327901
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
