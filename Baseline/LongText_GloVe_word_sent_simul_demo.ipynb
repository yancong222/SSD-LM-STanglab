{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import re\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'here'\n",
    "result = 'here'\n",
    "\n",
    "baseline = pd.read_csv(data_folder + 'here')\n",
    "baseline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ec7Q2J6vLLoE"
   },
   "source": [
    "# Install lib and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade gensim==3.8.3\n",
    "import string\n",
    "import math\n",
    "\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.models as gs\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbyAAHxdTFdR"
   },
   "source": [
    "# Get GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove embedding\n",
    "fname = get_tmpfile(\"here\")\n",
    "model = KeyedVectors.load_word2vec_format(fname)\n",
    "glove = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swords = stopwords.words('english')\n",
    "swords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [baseline, incoh10, incoh20, incoh50, ineff10, ineff20, ineff50]\n",
    "temp = -1\n",
    "for df in dfs:\n",
    "  temp += 1\n",
    "  for i in df.index:\n",
    "    df['content'][i] = df['content'][i].split(' ')\n",
    "  df = df.explode('content')\n",
    "  df['word_lower'] = df['content'].str.lower()\n",
    "  df[\"nltk.tokenized\"] = df.apply(lambda row: nltk.word_tokenize(str(row[\"word_lower\"]).lower()) , axis = 1)\n",
    "  df[\"nltk.n_token\"] = df.apply(lambda row: len(row[\"nltk.tokenized\"]) , axis = 1)\n",
    "  df[\"nltk.n_stopword\"] = df.apply(lambda row: len(list(filter(lambda x: x in swords, row[\"nltk.tokenized\"]))), axis = 1 )\n",
    "  df[\"is_stopword\"] = df.apply(lambda row: 1 if row[\"nltk.n_stopword\"] > 0 else 0, axis=1)\n",
    "  df.to_csv(result + str(temp) + '.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grid</th>\n",
       "      <th>content</th>\n",
       "      <th>n_words</th>\n",
       "      <th>word_lower</th>\n",
       "      <th>nltk.tokenized</th>\n",
       "      <th>nltk.n_token</th>\n",
       "      <th>nltk.n_stopword</th>\n",
       "      <th>is_stopword</th>\n",
       "      <th>glove_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13493</td>\n",
       "      <td>amount</td>\n",
       "      <td>134</td>\n",
       "      <td>amount</td>\n",
       "      <td>['amount']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.53754, 0.070699, -0.01096, 0.27205, -0.234...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13493</td>\n",
       "      <td>of</td>\n",
       "      <td>134</td>\n",
       "      <td>of</td>\n",
       "      <td>['of']</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.076947, -0.021211, 0.21271, -0.72232, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13493</td>\n",
       "      <td>depression</td>\n",
       "      <td>134</td>\n",
       "      <td>depression</td>\n",
       "      <td>['depression']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.27039, 0.30269, -0.53267, 0.34494, 0.5177,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13493</td>\n",
       "      <td>due</td>\n",
       "      <td>134</td>\n",
       "      <td>due</td>\n",
       "      <td>['due']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3376, -0.13567, -0.16164, -0.2616, -0.04275...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13493</td>\n",
       "      <td>to</td>\n",
       "      <td>134</td>\n",
       "      <td>to</td>\n",
       "      <td>['to']</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.25756, -0.057132, -0.6719, -0.38082, -0.36...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    grid     content  n_words  word_lower  nltk.tokenized  nltk.n_token  \\\n",
       "4  13493      amount      134      amount      ['amount']             1   \n",
       "4  13493          of      134          of          ['of']             1   \n",
       "4  13493  depression      134  depression  ['depression']             1   \n",
       "4  13493         due      134         due         ['due']             1   \n",
       "4  13493          to      134          to          ['to']             1   \n",
       "\n",
       "   nltk.n_stopword  is_stopword  \\\n",
       "4                0            0   \n",
       "4                1            1   \n",
       "4                0            0   \n",
       "4                0            0   \n",
       "4                1            1   \n",
       "\n",
       "                                           glove_emb  \n",
       "4  [-0.53754, 0.070699, -0.01096, 0.27205, -0.234...  \n",
       "4  [-0.076947, -0.021211, 0.21271, -0.72232, -0.1...  \n",
       "4  [-0.27039, 0.30269, -0.53267, 0.34494, 0.5177,...  \n",
       "4  [0.3376, -0.13567, -0.16164, -0.2616, -0.04275...  \n",
       "4  [-0.25756, -0.057132, -0.6719, -0.38082, -0.36...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_glove(r):\n",
    "  if type(r[\"word_lower\"]) != float and r[\"word_lower\"] in glove:\n",
    "    return glove[r[\"word_lower\"]] \n",
    "  else:\n",
    "    return np.nan\n",
    "\n",
    "baseline = pd.read_csv(result + '0.csv', index_col=0)\n",
    "incoh10 = pd.read_csv(result + '1.csv', index_col=0)\n",
    "incoh20 = pd.read_csv(result + '2.csv', index_col=0)\n",
    "incoh50 = pd.read_csv(result + '3.csv', index_col=0)\n",
    "ineff10 = pd.read_csv(result + '4.csv', index_col=0)\n",
    "ineff20 = pd.read_csv(result + '5.csv', index_col=0)\n",
    "ineff50 = pd.read_csv(result + '6.csv', index_col=0)\n",
    "\n",
    "dfs = [baseline, incoh10, incoh20, incoh50, ineff10, ineff20, ineff50]\n",
    "\n",
    "temp = -1\n",
    "for df in dfs:\n",
    "  temp += 1\n",
    "  df[\"glove_emb\"] = ''\n",
    "  df[\"glove_emb\"] = df.apply(lambda r: calc_glove(r), axis = 1)\n",
    "  df.to_csv(result + str(temp) + '.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['glove_emb'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['glove_emb'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    df.dropna(subset = ['glove_emb'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group together for sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = -1\n",
    "for df in dfs:\n",
    "    temp += 1\n",
    "    if temp == 0:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        baseline = df\n",
    "    if temp == 1:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        incoh10 = df\n",
    "    if temp == 2:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        incoh20 = df\n",
    "    if temp == 3:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        incoh50 = df\n",
    "    if temp == 4:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        ineff10 = df\n",
    "    if temp ==5:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        ineff20 = df\n",
    "    if temp == 6:\n",
    "        df['grid'] = df['grid'].astype(str)\n",
    "        df = df.groupby(['grid'], as_index=False).agg({'content': lambda x: ' '.join(x), \n",
    "        'glove_emb': lambda x: np.stack(x)})\n",
    "        df.to_csv(result + str(temp) + '_sent.csv')\n",
    "        ineff50 = df\n",
    "\n",
    "dfs = [baseline, incoh10, incoh20, incoh50, ineff10, ineff20, ineff50]\n",
    "incoh20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['glove_emb'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['glove_emb'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.grid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats and similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats ignoring nan\n",
    "from numpy import nanmedian\n",
    "\n",
    "import scipy\n",
    "def iqr(x):\n",
    "  return scipy.stats.iqr(np.array(x), nan_policy='omit')\n",
    "\n",
    "from numpy import nanquantile\n",
    "def q5(x):\n",
    "    return np.nanquantile(np.array(x), 0.05)\n",
    "\n",
    "def q95(x):\n",
    "    return np.nanquantile(np.array(x), 0.95)\n",
    "\n",
    "# cosine_similarity, apply to all LMs\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MV 5/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Alex', 'broke', 'the', 'vase', 'accidentally'], ['.', 'But', 'Kai', 'did', 'it'], ['on', 'purpose', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Average semantic similarity of each word in 5- or 10- words window\n",
    "\n",
    "def divide_chunks(l, n):\n",
    "      \n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]\n",
    "  \n",
    "# n: How many elements each\n",
    "# list should have\n",
    "test = ['Alex','broke','the','vase','accidentally','.','But','Kai','did','it','on','purpose','.']\n",
    "divide_chunks(test,5)\n",
    "chopped = list(divide_chunks(test,5))\n",
    "print(chopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Alex', 'broke'], ['Alex', 'the'], ['Alex', 'vase'], ['Alex', 'accidentally'], ['broke', 'the'], ['broke', 'vase'], ['broke', 'accidentally'], ['the', 'vase'], ['the', 'accidentally'], ['vase', 'accidentally']]\n"
     ]
    }
   ],
   "source": [
    "def combinations(lst): # get w1, w2 combinations\n",
    "    # input: a list of <= 5 tokens\n",
    "    cmb = []\n",
    "    rightside = lst[:] # initialize a list\n",
    "    for wid, w1 in enumerate(lst): # each token gets a chance to be w1\n",
    "        rightside = lst[wid:] # dynamically chop off w1 from the rest of the list\n",
    "        while rightside: # loop until the rest of the list is empty\n",
    "            w2 = rightside.pop(0) # stack up w2\n",
    "            if w2 != w1: # get rid of ['Alex', 'Alex']\n",
    "                cmb.append([w1, w2])  \n",
    "    return cmb\n",
    "\n",
    "testing = ['Alex', 'broke', 'the', 'vase', 'accidentally']\n",
    "test_result = combinations(testing)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvs=['5', '10']\n",
    "stats = ['_median', '_iqr', '_q5', '_q95'] # appended to the embeddings df\n",
    "for df in dfs:\n",
    "    # create new empty columns\n",
    "    for mv in mvs:\n",
    "        for stat in stats:\n",
    "            cur = 'glove_mv' + mv + stat\n",
    "            df[cur] = ''\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['glove_emb'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinations 'alex broke the vase': \n",
    "# alex broke, alex the, alex vase; broke the, broke vase; the vase\n",
    "# Parola et al is inspired by Pauselli et al (p76): \n",
    "# ''Coherence is the average similarity of each word to each of the other\n",
    "# words in the list, regardless of order or proximity. ''\n",
    "#w2 = word_embed[word_id+1] # adjacent neighbour only\n",
    "temp_file = -1\n",
    "for df in dfs:\n",
    "    temp_file += 1\n",
    "    print(temp_file)\n",
    "    for mv in mvs:\n",
    "        # print progress\n",
    "        cur = 'glove_mv' + mv\n",
    "        print('current: ', cur)\n",
    "        df[cur + '_similarity'] = '' # save the cosine similarities; all stats are derived from there\n",
    "\n",
    "        # loop over each response\n",
    "        for i in df.index:\n",
    "            if type(df['glove_emb'][i]) != float: \n",
    "                # chop 1 big response sequence into 5/10-token chunks\n",
    "                word_embed_chunk = list(divide_chunks(df['glove_emb'][i].tolist(), int(mv))) # add .tolist() bcs glove emb gives numpy.ndarray # add [0] bcs glove emb shape (1, 385, 300)\n",
    "                chunk_temp_collection = [] \n",
    "                # loop over each 5/10 chunk in the response\n",
    "                for chunck_id, word_embed in enumerate(word_embed_chunk):\n",
    "                    temp_collection = []\n",
    "                    # calculate average similarities for that chunk (5 or 10 window)\n",
    "                    cmbs = combinations(word_embed) # apply function \n",
    "                    for cmb in cmbs:\n",
    "                        w1 = cmb[0]\n",
    "                        w2 = cmb[1]\n",
    "                        temp = cosine_similarity(w1, w2)\n",
    "                        temp_collection.append(temp)\n",
    "                    temp_sim = np.nanmean(temp_collection)\n",
    "                    chunk_temp_collection.append(temp_sim) # incrementally append similarity mean to the list \n",
    "\n",
    "            # get a list of similarity means for that response, \n",
    "            # its len is the number of chunks that the response can be chopped into\n",
    "            df[cur + '_similarity'][i] = chunk_temp_collection # similarity mv 5 or 10; store it for later reference/stats\n",
    "\n",
    "            # add other stats here\n",
    "            df[cur + '_median'][i] = np.nanmedian(chunk_temp_collection)\n",
    "            df[cur + '_q5'][i] = q5(chunk_temp_collection)\n",
    "            df[cur + '_q95'][i] = q95(chunk_temp_collection)\n",
    "            df[cur + '_iqr'][i] = iqr(chunk_temp_collection)\n",
    "    df.to_csv(result + str(temp_file) + '_GloVe.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K1:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast # a module that evaluates mathematical expressions and statements\n",
    "\n",
    "ks=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "stats = ['_median', '_iqr', '_q5', '_q95']\n",
    "for df in dfs:\n",
    "    # create new empty columns\n",
    "    for k in ks:\n",
    "        for stat in stats:\n",
    "            cur = 'glove_k' + k + stat\n",
    "            df[cur] = ''\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = -1\n",
    "ks=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "for df in dfs:\n",
    "    temp_file += 1\n",
    "    # loop through each k\n",
    "    for k in ks:\n",
    "        cur = 'glove_k' + k \n",
    "        print('Coherence k ', k) # progress\n",
    "        df[cur + '_similarity'] = ''\n",
    "        # loop through each individual's response \n",
    "        for i in df.index:\n",
    "            if type(df['glove_emb'][i]) != float:\n",
    "                temp = []\n",
    "                # calcuate similarity of word pairs at k inter-token distance\n",
    "                for id,v in enumerate(df['glove_emb'][i]): # add [0] bcs glove emb shape (1, 385, 300)\n",
    "                    w1 = v\n",
    "                    try:\n",
    "                        w2 = df['glove_emb'][i][id + int(k)]\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "                    sim = cosine_similarity(w1, w2)\n",
    "                    temp.append(sim) # a list of similarity scores for that response\n",
    "\n",
    "                # intermediate df, save \n",
    "                df[cur + '_similarity'][i] = temp\n",
    "                df[cur + '_iqr'][i] = iqr(temp) # add other stats here\n",
    "                df[cur + '_median'][i] = np.nanmedian(temp)\n",
    "                df[cur + '_q5'][i] = q5(temp)\n",
    "                df[cur + '_q95'][i] = q95(temp)\n",
    "    df.to_csv(result + str(temp_file) + '_GloVe.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOC and SOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs=['foc', 'soc']\n",
    "stats = ['_median', '_iqr', '_q5', '_q95']\n",
    "for df in dfs:\n",
    "    # create new empty columns\n",
    "    for c in cs:\n",
    "        for stat in stats:\n",
    "            cur = 'glove_' + c + stat\n",
    "            df[cur] = ''\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = -1 # keep track of file names\n",
    "for df in dfs:\n",
    "    print('progress')\n",
    "    temp_file += 1\n",
    "    df['glove_foc_similarity'] = ''\n",
    "    df['glove_soc_similarity'] = ''\n",
    "    # loop over each response\n",
    "    for i in df.index:\n",
    "        temp_foc = [] # for each individual, store the list of cos similarity\n",
    "        temp_soc = []\n",
    "        if type(df['glove_emb'][i]) != float:\n",
    "            # calculate average similarity for sentence pairs, either adjacent or with one intervening\n",
    "            for idx, sent in enumerate(df['glove_emb'][i]):\n",
    "                try:\n",
    "                    temp_foc.append(cosine_similarity(sent, df['glove_emb'][i][idx+1])) # get a list of similarities for that response\n",
    "                    \n",
    "                    df['glove_foc_similarity'][i] = temp_foc # record intermediate similarities \n",
    "                    \n",
    "                    df['glove_foc_median'][i] = np.nanmedian(temp_foc) # add more stats here\n",
    "                    df['glove_foc_iqr'][i] = iqr(temp_foc)\n",
    "                    df['glove_foc_q5'][i] = q5(temp_foc)\n",
    "                    df['glove_foc_q95'][i] = q95(temp_foc)\n",
    "\n",
    "                    temp_soc.append(cosine_similarity(sent, df['glove_emb'][i][idx+2]))\n",
    "\n",
    "                    df['glove_soc_similarity'][i] = temp_soc # record intermediate similarities \n",
    "\n",
    "                    df['glove_soc_median'][i] = np.nanmedian(temp_soc)\n",
    "                    df['glove_soc_iqr'][i] = iqr(temp_soc)\n",
    "                    df['glove_soc_q5'][i] = q5(temp_soc)\n",
    "                    df['glove_soc_q95'][i] = q95(temp_soc)\n",
    "\n",
    "                except IndexError:\n",
    "                    continue\n",
    "    print('finished')\n",
    "    df.to_csv(result + str(temp_file) + '_Glove.csv')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rdfu-cu9_CEY"
   ],
   "machine_shape": "hm",
   "name": "ACES_basic_gt_processing.ipynb",
   "provenance": [
    {
     "file_id": "1ETF3x6oEJywFFsX0tCKRuVjBHgxh8Qrd",
     "timestamp": 1637704592379
    },
    {
     "file_id": "13oLsCUJOgMCcnuSIFhBO0gX9CGSJdTQB",
     "timestamp": 1637691259779
    },
    {
     "file_id": "1FQh9dMlL6_Wg-A5-kxq7oYMTF_cstmUE",
     "timestamp": 1637248901827
    },
    {
     "file_id": "1ogy3oiNfRRjrrZUlOsMUxo5CxKM-xR0o",
     "timestamp": 1617996327901
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
